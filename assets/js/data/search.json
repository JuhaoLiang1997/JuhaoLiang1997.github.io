[ { "title": "关于自监督学习模型的随笔", "url": "/posts/%E5%85%B3%E4%BA%8E%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9A%8F%E7%AC%94/", "categories": "Technical", "tags": "BERT, NLP, Transfer Learning", "date": "2022-05-16 20:38:00 +0800", "snippet": "关于自监督学习模型的随笔吃完饭休息一会，看李宏毅老师视频做的随笔参考资料：惡搞自督導式學習模型 BERT 的三個故事, Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation ModelOverview视频讲述了 BERT 三种有意思的研究： Cross-lingual Cross-discipline Pre-training with artificial data第一点是关于多语言 BERT 的跨语言能力，例如不同语言通过向量转换进行的字符级翻译，这个是符合直觉的方法，而且是很有意思的研究。第二点是关于 BERT 跨学科能力，感觉是从 cross-lingual 进阶而来，大概思路就是将其他任务输出为伪 “token” 形式，与已有的语言 token 做一个映射，从而利用了模型中语言预训练的信息。第三点是尝试使用不同规则下人为制造的信息进行预训练，查看对不同任务的提升效果。总的来说，这三点是层层递进的，主要做的也都是做迁移学习 transfer learning，利用不同领域的数据进行互相协助。Transfer Learning + Multimodal machine learning is promising.Cross-lingual Capability of BERT多语言 BERT 的 跨语言能力，只用英文数据集 SQuAD fine tune 的模型在中文问答数据集 DRCD 有不俗的效果。 XTREME: Cross-lingual TRansfer Evaluation of Multilingual Encoders benchmark 用来测试跨语言表达能力的数据集Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model通过一个 bi-lingual dictionary 得到中英字符对应关系，在 BERT 中计算中英字符各自的 embedding（corpus平均），再计算中英对应关系字符的 embedding similarity，做 ranking，得到 Mean Reciprocal Rank (MRR)，越高 MRR 表示越好的跨语言表示能力。Mean Reciprocal Rank将 Multi-lingual BERT 所有中文符号 embedding 和英文符号 embedding 分别做平均，得到两个语言的 average embedding，两个语言的平均向量的差可以近似的理解为两个语言的转换方式，例如 embedding(fish) + embedding(中文) - embedding(English) = embedding(鱼)。Cross-discipline Capability of BERTBERT 的跨学科能力，在人类语言学习的知识对其他学科有一定的帮助，例如将 DNA 序列元素一一映射到语言元素之后，使用语言数据集预训练的 BERT 对 DNA 分类有一定的帮助。通过语言数据的预训练，对其他学科的应用是有帮助的 both Optimization and Generalization.BERT 的划学科能力对 Speech Question Answering 有帮助。Speech Question Answering 任务就是输入一段语音，输出答案所在位置。同样是将语音的 token 和 语言的 token 一一对应，再接上语言数据集预训练的模型，可以有效提高性能。pretrained speech model + pretrained text BERTPre-training with Artificial Data of BERT除了语言数据，在基于规则的人造数据上预训练 BERT。如果是随机生成的人造数据，则对实际任务中并无作用；而如果有一定规律的（成对出现），则会有一定的帮助；shuffle 打乱连续编号序列，也会对结果有帮助。" }, { "title": "Semi-supervised", "url": "/posts/semi-supervised/", "categories": "Technical", "tags": "Semi-supervised Learning", "date": "2022-05-15 20:32:00 +0800", "snippet": "Semi-supervised Learning简单了解一下 semi-supervised learning。参考资料：ML Lecture 12: Semi-supervised, A Survey on Deep Semi-supervised LearningOverview半监督学习（semi-supervised learning）指的是大部分训练数据是未标注的，少部分已标注。其中分为 transductive learning 和 inductive learning，前者表示未标注数据是测试数据，且参与训练过程；后者是指未标注数据不是测试数据，测试数据不参与模型训练。为什么需要半监督学习呢？因为标注数据工作是很昂贵的，半监督学习可以由部分已知数据推断未标注数据。半监督学习通常会伴随着一些假设 assumptions，而这些 assumptions 的质量决定了半监督学习的有效性。Semi-supervised Learning for Generative Model在传统 supervised learning 中添加 unlabeled data 影响最终 loss 的计算，需要 solved iteratively 因为不是 convex problem。\\[P_{\\theta}\\left(x^{u}\\right)=P_{\\theta}\\left(x^{u} \\mid C_{1}\\right) P\\left(C_{1}\\right)+P_{\\theta}\\left(x_{0}^{u} \\mid C_{2}\\right) P\\left(C_{2}\\right)\\]Steps Initialization: \\(\\theta=\\left\\{P\\left(C_{1}\\right), P\\left(C_{2}\\right), \\mu^{1}, \\mu^{2}, \\Sigma\\right\\}\\) Step1: compute the posterior probability of unlabeled data: \\(P_{\\theta}\\left(C_{1} \\mid x^{u}\\right)\\) 计算 unlabeled data 的属于某一类的概率 Step2: update model. 更新时，P(C1) 会加上所有 unlabeled data 属于 C1 的和，再更新 $\\mu$，重复此步骤。 N: total number of examples, N1: number of examples belonging to C1\\[P\\left(C_{1}\\right)=\\frac{N_{1}+\\sum_{x^{u}} P\\left(C_{1} \\mid x^{u}\\right)}{N}\\]\\[\\mu^{1}=\\frac{1}{N_{1}} \\sum_{x^{r} \\in C_{1}} x^{r}+\\frac{1}{\\sum_{x^{u}} P\\left(C_{1} \\mid x^{u}\\right)} \\sum_{x^{u}} P\\left(C_{1} \\mid x^{u}\\right) x^{u}\\] Back to step 1 The algorithm converges eventually, but hte initialization influences the results. Low-density Separation Assumption非黑即白。Self-training给定 labelled data 和 unlabeled data，用 labelled data 训练得到模型，再用训练得到的模型预测 unlabeled data 得到 pseudo-label，再用部分 unlabeled data 参与训练模型，利用更新的模型继续预测剩下的 unlabeled data，循环运作。和 generative model 的区别是 self-training 使用的 hard label，而 generative model 用的是 soft label (probability)。soft label 对 neural network 其实是没有用的，hard label 对 regression model 是没有用的。Entropy-based Regularization相当于在 loss function 中添加表示 unlabeled 不确定程度的 regularization，使得模型在未标注数据的标签预测中更加自信。用 entropy 计算 预测分布是否集中：\\(E\\left(y^{u}\\right)=-\\sum_{m=1}^{5} y_{m}^{u} \\ln \\left(y_{m}^{u}\\right)\\) as small as possibleEntropy-based regularization loss functionSemi-supervised SVM对未标注数据所有标签的可能都做一次 SVM，判断哪一种情况下 margin 最大且 error 最小，从而选择作为最终 model。Smoothness Assumption近朱者赤，近墨者黑。假设 x1 和 x2 在同一片集中区域，可以认为 y1 和 y2 是相同的，下图中 x2 更趋向于与 x1 有相同的标签。Smoothness AssumptionCluster and then Label大概思路就是用有标注数据+无标注数据去做 clustering，通过聚类结果再给无标注数据标签。需要有强聚类方法才会 work。Graph-based Approach利用所有数据建立 graph，两两之间计算 similarity，再根据数据特征 (heuristic) 建连边。例如使用 K Nearest Neighbour 或 e- Neighbourhood 建连边。又或者使用加权连边，Gaussian Radial Basis Function。The labelled data influence their neighbors, propagate through the graph. 标签信息会随着连边传播。定义 graph 的 label smoothness，图的标签是否顺滑 (渐变)：计算每条连边的两端节点标签相同的个数，越多相同表示越顺滑，反之则 low smoothness。Graph Laplacian: L = D - W, D是每个节点的连边权重和，W是节点之间的连边权重Label Smoothiness最后在 loss function 加上 label smoothiness 值，使得未标注数据的预测结果是 smooth 的。Better Representation去芜存菁，化繁为简。Find the latent factors behind the observation. The latent factors (usually simpler) are better representations。" }, { "title": "Transfer Learning", "url": "/posts/transfer-learning/", "categories": "Technical", "tags": "Transfer Learning", "date": "2022-05-14 20:53:00 +0800", "snippet": "Transfer Learning简单了解一下迁移学习（Transfer Learning）。参考文献：ML Lecture 19: Transfer Learning, Transfer LearningOverview根据 target data、source data（是否带标签）区分为四种类型：Model Fine-tuningTarget data 和 Source data 都有标签时，可以用 Model fine-tuning 做迁移学习。基本思路就是用 source data 先训练模型，再用 target data 微调模型。One-shot learning当 target data 数量少的时候，可以称为 One-shot learning: only a few examples in target domain。Conservative Training训练时加入 constraint 防止 target data 过少导致的 overfitting。Layer TransferLayer transfer：直接复制 source data 训练模型的部分 layer。只用 target data 训练其中几层，可以防止 overfitting。如何选择复制哪些层？ 语音上通常复制最后几层，表示更深层次的内容，不同个体发声方式不一样。 图像中通常复制前面几层，表示更简单的图像构成，根据训练任务不同提取内容。Multitask LearningTarget data 和 Source data 都有标签时，也可以用 Multitask learning 做迁移学习。基本思路就是一个模型（部分 component）同时训练多个任务。可以处理例如多语言辨识的任务。Multitask LearningProgressive Neural NetworkDomain-adversarial trainingTarget data 无标签，Source data 有标签时，可以用 Domain-adversarial training 做迁移学习。目标在于消除 Target data 和 Source data 的差异。和 GAN 相似，由三个模型组成，feature extractor model 用于提取不同数据集数据的 latent feature，label predictor 根据 latent feature 区分预测标签，Domain classifier 根据 latent feature 判断数据来自于哪个数据集。Domain-adversarial training is a big network, but different parts have different goalsZero-shot LearningTarget data 无标签，Source data 有标签时，也可以用 Zero-shot learning 做迁移学习。基本思路是提取两个任务中更底层的共有属性。例如在图像分类中，提取出各个动物的属性，再用模型预测是否拥有各个属性（毛发、四肢、尾巴），根据预测结果（attribute embedding）再推测属于哪个动物。attribute 可以用 word embedding 表示。Data Attributes ExampleConvex Combination of Semantic Embedding根据 source data 训练模型得到的预测结果，各个结果的概率再结合 word embedding 加权求和得到新的动物的表示（例如：0.5 老虎 + 0.5 狮子=狮虎兽）。Self-taught Learning &amp;amp; Self-taught ClusteringTarget data 有标签，Source data 无标签时，可以用 Self-taught Learning 做迁移学习。有点像 semi-supervised learning，但两个数据集是不完全一样的。基本思路就是用无标签数据提取 better representation。Target data 无标签，Source data 也无标签时，可以用 Self-taught Clustering 做迁移学习。思路和 self-taught learning 差不多，也是预先提取 representation。" }, { "title": "BERT Model", "url": "/posts/bert-model/", "categories": "Technical", "tags": "tools, BERT", "date": "2022-04-02 03:29:00 +0800", "snippet": "BERT Model基于 Huggingface Transformer实战教程 的 BERT 模型笔记，写的有点杂，回头再整理补充一下。课程相关课程目标《Huggingface Transformers实战教程 》是专门针对HuggingFace开源的transformers库开发的实战教程，适合从事自然语言处理研究的学生、研究人员以及工程师等相关人员的学习与参考，目标是阐释transformers模型以及Bert等预训练模型背后的原理，通俗生动地解释transformers库的如何使用与定制化开发，帮助受众使用当前NLP顶级模型解决实际问题并取得优秀稳定的实践效果。BERTBERT 框架BERT整体框架包含 pre-train 和 fine-tune 两个阶段。pre-train 阶段模型是在无标注的标签数据上进行训练，fine-tune 阶段，BERT模型首先是被 pre-train 模型参数初始化，然后所有的参数会用下游的有标注的数据进行训练。BERT是用了 Transformer 的 encoder 侧的网络，encoder中的 Self-attention 机制在编码一个token的时候同时利用了其上下文的 token，其中‘同时利用上下文’即为双向的体现，而并非想 Bi-LSTM 那样把句子倒序输入一遍。在它之前是 GPT，GPT使用的是 Transformer 的 decoder 侧的网络，GPT是一个单向语言模型的预训练过程，更适用于文本生成，通过前文去预测当前的字。EmbeddingEmbedding由三种 Embedding 求和而成： Token Embeddings 是词向量，第一个单词是 CLS 标志，可以用于之后的分类任务 Segment Embeddings 用来区别两种句子，因为预训练不光做 LM 还要做以两个句子为输入的分类任务 Position Embeddings 和之前文章中的 Transformer 不一样，不是三角函数而是学习出来的特殊 tokens [CLS]：在做分类任务时其最后一层的repr. 会被视为整个输入序列的repr. 一般会被放在输入序列的最前面 [SEP]：有两个句子的文本会被串接成一个输入序列，并在两句之间插入这个 token 以做区隔 [UNK]：没出现在 BERT 字典里头的字会被这个 token 取代 [PAD]：zero padding 遮罩，将长度不一的输入序列补齐方便做 batch 运算 [MASK]：未知遮罩，仅在预训练阶段会用到，一般在 fine-tuning 或是 feature extraction 时不会用到，这边只是为了展示预训练阶段的遮蔽字任务才使用的。 tokenizer.all_special_ids # 特殊 token 的 idtokenizer.all_special_tokens # 特殊 token Transformer Encoder在 Transformer 中，模型的输入会被转换成 512 维的向量，然后分为 8 个head，每个 head 的维度是 64 维，但是 BERT 的维度是 768 维度，然后分成 12 个 head，每个 head 的维度是 64 维，这是一个微小的差别。Transformer 中 position Embedding 是用的三角函数，BERT 中也有一个 Postion Embedding 是随机初始化，然后从数据中学出来的。BERT 模型分为 24 层和 12 层两种，其差别就是使用 transformer encoder 的层数的差异，BERT-base 使用的是 12 层的 Transformer Encoder 结构，BERT-Large 使用的是 24 层的 Transformer Encoder 结构。Tokenizer tokenizer.encode(text) 只返回 input_ids tokenizer.encode_plus(text) 返回所有的 embedding 信息，包括 input_ids： token embeddings token_type_ids：segment embeddings attention_mask：指定 token 做 self-attention Model 输出 pooler_output 对应的是 [CLS] 的输出 (batch_size, hidden_size) last_hidden_state 对应的是序列中所有 token 最后一层的 hidden 输出 (batch_size, sequence_length, hidden_size) hidden_states (optional) 如果输出，需要指定config.output_hidden_states=True ，它是一个元组，它的第一个元素是embedding，其余元素是各层的输出，每个元素的形状是(batch_size, sequence_length, hidden_size) attentions (optional) 如果输出，需要指定config.output_attentions=True，它是一个元组 ，它的元素是每一层的注意力权重，用于计算self-attention heads的加权平均值实战相关随机种子def seed_torch(seed=42): random.seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = Truenp.random.seed(CONFIG.SEED)seed_torch(seed=CONFIG.SEED)数据统计train.info() # 查看缺失值train[train[&#39;content&#39;].isna()] # 查看缺失值所在数据train[&#39;label&#39;].value_counts() # 标签分布train[&#39;content&#39;]=train[&#39;content&#39;].fillna(&#39;空值&#39;) # 数据预处理train[&#39;text&#39;].map(len).describe() # 文本长度统计train[&#39;text&#39;].nunique() == train.shape[0] # 判断 unique text自定义数据集class CustomDataset(Dataset): def __init__(self, texts, labels, tokenizer, ...): self.texts = texts, ... def __len__(self): return len(self.texts) # 数据个数 def __getitem__(self, item): # item 是数据索引，处理第 item 条数据 encoding = self.tokenizer.encode_plus(...) # 可以在 dataset 里做 tokenization return {&#39;texts&#39;: text, &#39;encoding&#39;: ...}train, test = train_test_split(dataset, ....) # 切割训练集测试集dataloader = DataLoader(dataset, batch_size=...) # dataloader 分 batch训练Optimizer AdamW (Adam with decoupled weight decay) : 效果更好 huggingface AdamSchedulerget_linear_schedule_with_warmup ： adaptive learning rateclip_grad_norm_nn.utils.clip_grad_norm(parameters, max_norm, norm_type=2) 设置一个梯度剪切的阈值，如果在更新梯度的时候，梯度超过这个阈值，则会将其限制在这个范围之内，防止梯度爆炸。 parameters (Iterable[Tensor] or Tensor): 参数 max_norm (float or int) 梯度的最大范数 norm_type(float or int)=2.0 规定范数的类型LossBCEWithLogitsLoss = Sigmoid+BCELoss，当网络最后一层使用nn.Sigmoid时，就用BCELoss，当网络最后一层不使用nn.Sigmoid时，就用BCEWithLogitsLoss。(BCELoss) BCEWithLogitsLoss 用于单标签二分类或者多标签二分类，输出和目标的维度是(batch_size,n_class)，batch_size 是样本数量，n_class 是类别数量，对于每一个 batch 的 n_class 个值，对每个值求 sigmoid 到 0-1 之间，所以每个 batch 的 n_class 个值之间是没有关系的，相互独立的，所以之和不一定为1。每个C值代表属于一类标签的概率。如果是单标签二分类，那输出和目标的维度是 (batch_size,1) 即可。保存模型if val_acc &amp;gt; best_acc: torch.save(model.state_dict(), &#39;best_model_state.bin&#39;) best_acc = val_acc# model.load_state_dict(torch.load(&#39;best_model_state.bin&#39;))任务 02-文本分类实战：基于Bert的企业隐患排查分类模型 分类任务：使用 bert 模型的 pooled_output 接入 linear layer 进行分类，pooled_output 是整体句子表示 def forward(self, input_ids, attention_mask): _, pooled_output = self.bert( input_ids=input_ids, attention_mask=attention_mask, return_dict = False ) output = self.drop(pooled_output) # dropout return self.out(output) # linear layer (hidden_size, n_classes) 03-文本多标签分类实战：基于Bert对推特文本进行多标签分类 多分类就是 bert 模型接一个 linear layer 进行分类 多分类使用 BCEWithLogitsLoss ，BCEWithlogitsloss=sigmoid+BCELoss pytorch官方为了数值计算稳定性，将 sigmoid 层和 BCELoss 合并到了一起，当网络最后一层使用nn.Sigmoid时，就用BCELoss，当网络最后一层不使用nn.Sigmoid时，就用BCEWithLogitsLoss。 预测阶段就用 sigmoid + threshold (0.5) 得出预测的多标签 验证集中使用 f1_score 搜索范围内的 thresholds，首先用大范围找 range(1, 10)/10，找到粗略的范围后再精细的找 range(10)/100，找到最佳的 threshold 04-句子相似性识别实战：基于 Bert 对句子对进行相似性二分类 二分类任务，bert_pooler_output + dropout + Linear (hidden_size, 1) 自动混合精度训练，大概意思就是自动调整精度 torch.float64/32 在有的情景下能加速运算和降低内存占用。 单标签二分类用 BCELoss / BCEWithLogitsLoss 可根据数据集调整 threshold 05-命名实体识别实战：基于预训练模型进行商品实体识别微调 seq2seq 问题，句子生成 label sequence，每一个 token 分别做分类 BERT做NER 一个棘手部分是 BERT 依赖于 wordpiece tokenization，而不是 word tokenization。比如：Washington的标签为 “b-gpe”,分词之后得到， “Wash”, “##ing”, “##ton”,”b-gpe”, “b-gpe”, “b-gpe” 给各个 subword 添加同样的 label 算准确率的时候还得把 mask 的移除 torch.masked_select 06-多项选择任务实战：基于Bert实现SWAG常识问题的多项选择 多项选择题，给出情景，给一句话的开头，多个续接选项中选一个 数据预处理就是把上下文、开头和各个选项拼在一起，训练就是各个选项拼接在一起，做分类问题。 transformers 库里有 AutoModelForMultipleChoice 的类 这个实战讲解了使用 huggingface_hub 的相关操作 07-文本生成实战：基于预训练模型实现文本生成 模型: AutoModelForCausalLM seq2seq 问题，取各个 token 对应的 last_hidden_state 做词表 softmax 08-文本摘要实战：基于预训练模型实现文本摘要任务 gpt-2: pipe = pipeline(&quot;text-generation&quot;, model=&quot;gpt2-xl&quot;) t5: pipe = pipeline(&quot;summarization&quot;, model=&quot;t5-large&quot;) bart: pipe = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;) pegasus: pipe = pipeline(&quot;summarization&quot;, model=&quot;google/pegasus-cnn_dailymail&quot;) 09-文本翻译实战：基于Bert实现端到端的机器翻译 翻译任务 WMT dataset 模型：AutoModelForSeq2SeqLM 10-问答实战：基于预训练模型实现QA 基于 context 的问答 模型：AutoModelForQuestionAnswering Huggingface 相关Pipeline Transformers 库中最基本的对象是pipeline()函数。它将模型与其必要的预处理和后处理步骤连接起来，使我们能够直接输入任何文本并获得答案：流程：Tokenizer -&amp;gt; Model -&amp;gt; Post-Processing常用 pipeline: &quot;feature-extraction&quot;: will return a FeatureExtractionPipeline. &quot;text-classification&quot;: will return a TextClassificationPipeline. &quot;sentiment-analysis&quot;: (alias of “text-classification”) will return a TextClassificationPipeline. &quot;token-classification&quot;: will return a TokenClassificationPipeline. &quot;ner&quot; (alias of “token-classification”): will return a TokenClassificationPipeline. &quot;question-answering&quot;: will return a QuestionAnsweringPipeline. &quot;fill-mask&quot;: will return a FillMaskPipeline. &quot;summarization&quot;: will return a SummarizationPipeline. &quot;translation_xx_to_yy&quot;: will return a TranslationPipeline. &quot;text2text-generation&quot;: will return a Text2TextGenerationPipeline. &quot;text-generation&quot;: will return a TextGenerationPipeline. &quot;zero-shot-classification&quot;: will return a ZeroShotClassificationPipeline. &quot;conversational&quot;: will return a ConversationalPipeline.预训练模型架构 huggingface Auto Classes" }, { "title": "Transformer: Attention Is All You Need", "url": "/posts/transformer-attention-is-all-you-need/", "categories": "papers", "tags": "Transformer", "date": "2022-03-31 21:42:00 +0800", "snippet": "Paper 阅读笔记Attention Is All You Need, 【機器學習2021】Transformer, transformer implementation, 代码笔记, Transformer 面经总结transformer 主要还是利用了 self-attention 的机制，打破了传统 rnn 基于序列的线性训练方法，增强其并行运算能力，克服长距离依赖问题；但与此同时，局部信息的获取没有 RNN 和 CNN 的强。个人笔记Transformer 结构宏观上来讲， transformer 由两部分组成 encoder + decoder。以文本翻译为例（英译中），encoder 负责英文原句子的意思理解，decoder 负责用 encoder 所理解的整体句子意思逐字的生成中文句子。Encoder 流程 输入表示向量 inputs (seq_len, d_model): seq_len 最大 sequence 长度，d_model 维特征向量（词向量） 添加 position encoding: inputs_pos (seq_len, d_model) = inputs (seq_len, d_model) + positional encoding (seq_len, d_model) 这里 positional encoding 可以用公式计算出来，或者用可训练的（bert）； 直接相加和 concat 到后面没什么区别，不影响原本的向量表达 position encoding公式 Self Attention的部分，$W_{q}, W_{k}, W_{v}$ 是三个可训练的权重矩阵，用于将 inputs_pos 转换成 Query, Key, Value，Query Matrix, Key Matrix, Value Matrix 每一行代表一个 token 的 Query, Key, Value QueryMatrix(seq_len, d_k) = inputs_pos(seq_len, d_model) * Wq(d_model, d_k)KeyMatrix(seq_len, d_k) = inputs_pos(seq_len, d_model) * Wk(d_model, d_k)ValueMatrix(seq_len, d_v) = inputs_pos(seq_len, d_model) * Wv(d_model, d_v) Query, Key, Value Matrix计算 求得 Query 和 Key 的点积，表示当前 token 对 所有 tokens 的 attention。如果特征维度 d_model 很大的话，这里点积结果会很大，需要做 scaling 保持其 variance 为 1，使得 softmax 结果差距不会太大，从而解决梯度消失的问题。参考 AttentionMatrix(seq_len, seq_len) = QueryMatrix(seq_len, d_k) * KeyMatrix.T(d_k, seq_len) / (d_k)^0.5 将 attention 按行做 softmax，将 attention 压缩到 0 到 1 之间 AttentionMatrix(seq_len, seq_len) = softmax(AttentionMatrix, axis=0) 利用计算得到的 attention 点乘 value，再相加，得到 outputs layer_outputs(seq_len, d_v) = AttentionMatrix(seq_len, seq_len) * ValueMatrix(seq_len, d_v) 步骤 3-6 是计算单个 Self-Attention 输出的过程。实际上 Multi-Head Attention 是将步骤 3 中 QueryMatrix, KeyMatrix, ValueMatrix 按照特征维度 (d_model) 切割成多个子集，每个子集的特征维度是 (d // n_head)。将切割后的 QKV Matrix 进行 步骤456 Self-Attention 计算得到各自的 layer_outputs，再 concat 在一起，构成输出 (seq_len, d_v)，最后加个 linear transformer 将 (seq_len, d_v) 转为 (seq_len, d_model) 就好。 subQueryMatrixs, subKeyMatrixs, subValueMatrixs = split(QueryMatrix), split(KeyMatrix), split(ValueMatrix)subWeightedValues = softmax(scale(subQueryMatrixs * subKeyMatrixs)) * subValueMatrixsconcated_outputs(seq_len, d_v) = concat([subWeightedValue1, subWeightedValue1, ...]) transformed_outputs(seq_len, d_model) = concated_outputs(seq_len, d_v) * linear_transformer(d_v, d_model) 步骤 7 得到了 Multi-Head Attention 的结果后，需要做一个残差连接，与 input_pos 进行 Residual &amp;amp; Norm &amp;amp; Dropout 的操作 add_outputs(seq_len, d_model) = transformed_outputs(seq_len, d_model) + inputs_pos(seq_len, d_model)multihead_outputs(seq_len, d_model) = Dropout(LayerNorm(add_outputs)) Batch Norm 是对每一个维度进行 normalization，Layer Norm 是对单个 token 的特征向量进行 normalization，Layer Norm 会消除同一特征的差异性，一般 Batch Norm 用于图像，Layer Norm 用于NLP，而 NLP 的 embedding 一般计算 cos similarity 作为相似度，所以单一特征差异性其实不关键。 Batch Normalization 这里到了 Feed-Forward 的部分了，利用俩全连接层将 multihead_outputs 提炼到高维空间 (论文中维度提高了四倍)，利用 ReLU 进行激活，原理和 SVM 差不多，将低维特征映射到高维更容易区别特征差异。上一步加 Layer Norm 也是为了在这一步激活函数更好的发挥作用 latent_represent(seq_len, latent_d) = Dropout(ReLU(multihead_outputs * w1 + b1))ff_output(seq_len, d_model) = latent_represent * w2 + b2 Feed-Forward network 同步骤 8，给 Feed-Forward Network 再加一个 Residual &amp;amp; Norm &amp;amp; Dropout，作用一是解决梯度消失的问题，二是解决权重矩阵的退化问题 步骤 3-10 是一个 encoder 部分需要重复的 network，论文中重复了 $N=6$ 次。至此就完成了 encoder 的部分了，也就是 transformer 结构图中左半部分，输出结果是和输入特征维度一致的 (seq_len, d_model)。 Decoder 流程 输入 output 的表示向量 outputs (seq_len, d_model): seq_len 个 tokens，d_model 维特征向量。 seq_len 是由最大 sequence 长度决定的，短的 sequence 由 &amp;lt;mask&amp;gt; 做 padding 训练阶段数据就是 target sequence 偏移一位: [&amp;lt;/s&amp;gt;, token1, token2, ...] ，这个偏移一位的目的是为了接下来的mask比较好做。各个 predict element 的生成是并行的 预测阶段（生成阶段）是需要多次调用 decoder 的部分的，在上一个 decoder 生成输出之后，如果不为结束符，则加入到 predicted_data 里 [\\&amp;lt;/s&amp;gt;, token1, token2, ...]，继续扔进 decoder 里预测下一个 token。各个 predict element 的生成是串行的。 加入 position encoding 信息，同 encoder 步骤 2。 这一步的 Multi-Head Attention 用的是 masked 的，就是 encoder 步骤 4 中将还没有 predict 的部分的 attention 给 mask 掉，这样就可以在那一步只获取之前的 weighted value，而丝毫不受未预测部分的 value 所影响；而至于在后面那个 Multi-Head Attention 为什么不用 mask？是因为那一步中的 key 和 value 是 encoder 的结果，包含的是整个序列的信息，与未预测的真实信息无关，所以不用 mask。其余的同 encoder 的 步骤 3-8。 接下来的一个 Multi-Head Attention 使用了 encoder 的结果，Query 是用上一个 layer 的结果求得的，Key 和 Value 是用 encoder 的结果求得的，Residual + LayerNorm + Dropout，得到 multihead_outputs(seq_len, d_model) QueryMatrix(seq_len, d_k) = first_MultiLayer_output(seq_len, d_model) * Wq(d_model, d_k)KeyMatrix(seq_len, d_k) = encoder_output(seq_len, d_model) * Wk(d_model, d_k)ValueMatrix(seq_len, d_v) = encoder_output(seq_len, d_model) * Wv(d_model, d_v) AttentionMatrix(seq_len, seq_len) = QueryMatrix(seq_len, d_k) * KeyMatrix.T(d_k, seq_len) / (d_k)^0.5 layer_outputs(seq_len, d_v) = AttentionMatrix(seq_len, seq_len) * ValueMatrix(seq_len, d_v) FF layer 也是同 encoder 步骤9-10。 步骤 3-5 也是要重复 $N$ 次。 最后再 Linear tranformation 降为 token corpus 大小，softmax 求各个 token 的概率。 小细节Regularization Residual Dropout: 每一层前面都加了 dropout function，包括 encoder 和 decoder 求 inputs_pos Residual Dropout Label Smoothing: cross-entropy 中使标签平滑 Label Smoothing 权重共享 Encoder 和 Decoder 间的 Embedding 层权重共享； 对于共有的一些 tokens 可以更好的表达；但是词表会很大 Decoder 中 Embedding 层和 FC 层权重共享 FC 层通过得到的向量，根据 embedding 层的权重反向求得每个 token 的概率 fc = nn.Linear(d, v, bias=False) # Decoder FC层定义，无 biasweight = Parameter(torch.Tensor(out_features, in_features)) # nn.Linear 的权重部分定义 Mask用到的地方 因为所有的文本长度不一样，所以会有一些文本是经过padding处理的，padding 部分用 mask 遮盖 decoder 计算 attention 的时候，未预测部分由 mask 遮盖 Multi-Head Attention 多头注意力机制是按照特征维度划分的，可以保证计算量不增加的情况下，提升特征捕捉能力 Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 可以类比CNN中同时使用多个滤波器的作用，直观上讲，多头的注意力有助于网络捕捉到更丰富的特征/信息。 decoder 训练和测试的输入输出差别 训练阶段中，decoder 的输入是真实输出序列向右偏移一位: [&amp;lt;\\s&amp;gt;, token1, token2, ...]，在 decoder 的第一个 Multi-Head Attention 中生成 query-key attention 之后对未预测部分做了 mask，如以下 AttentionMatrix 表格，目的是为了预测下一个 token 时只用到当前及之前的 token 信息，例如预测 token1 时只用到了 &amp;lt;\\s&amp;gt; 一个 token。那么为什么 decoder 中第二个 Multi-Head Attention 不用 mask 呢？是因为第二个 mask 的 key, value 都是 encoder 生成的，带着整个 sequence 的信息，用于生成下一个 token 是没问题的；在剩下的操作中 (linear transformer &amp;amp; normalization) 都是按行处理，因此不会发生数据泄露。 query\\key &amp;lt;\\s&amp;gt;.key token1.key token2.key token3.key mask.key (padding) &amp;lt;\\s&amp;gt;.query attention mask mask mask mask token1.query attention attention mask mask mask token2.query attention attention attention mask mask token3.query attention attention attention attention mask mask.query (padding) mask mask mask mask mask 训练阶段中，decoder 的输出应该是 (seq_len, k)，每一行都是独立预测出来的，没有依赖前一个 token 的预测，例如预测 token5 是由真实 token1 到真实 token4 所生成的。loss function 就是 predictMatrix (seq_len, vocab_size) 和 truthMatrix (seq_len, vocab_size) 的 cross-entropy，其中 truthMatrix 使用的不是 onehot，是 label smoothing。 测试阶段中，decoder 的输入是 sequential 的，即每个 token 按先后顺序预测，根据已预测的 token 去预测新的 token，例如预测第一个 token 的输入应该是: [&amp;lt;\\s&amp;gt;, mask, ...] 长度固定，输出应该就是: [token1, ....]。再预测下一个的时候直到预测到结束符就可以停下来了，这时就得到了完整的预测序列了。 论文笔记Introduction: In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.Model Architecture:Encoder and Decoder StacksEncoder: Encoder 部分是由 $N=6$ 个相同的 layer 组成，每一个 layer 有两个部分，第一个部分是 multi-head self-attention，第二个部分是一个简单的全连接层。两个部分都用了残差连接 + layer normalization，即 $Output_{sub-layer} = LayerNorm(x+Sublayer(x))$ ，每一个输出的维度都相同 $d_{model} = 512$ 。Decoder: Decoder 部分也是由 $N=6$ 个相同的 layer 组成，与 encoder 不同的是每个 layer 中有一个额外的 Masked Multi-Head Attention Sub-layer，这里的 Mask 是用来遮盖待预测的序列的，只允许使用之前的序列进行预测；而第二个 Multi-Head Attention 是和 encoder 输出部分 concat 之后一起做 self-attention。AttentionAttention 的功能是把一组 input 映射成对应的 query, key, value，利用 query 和各个输入的 key 求出对各个 input 的权重，再根据权重求出 weighted value 的集成，得到最后 query 对应 input 的 output：left: Scaled Dot-Product Attention, right: Multi-Head AttentionScaled Dot-Product Attention上图左边是 Scaled Dot-Product Attention，输入是 k 维的 query, key 和 v 维的 value，计算 query 和所有 key 的点积再除以 $\\sqrt{d_{k}} $ ，在用 softmax 取得各个 value 的权重。式子如下：\\[\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V\\]有两种常用的 attention: dot-product attention and additive attention，这里就是用的 dot-product attention + scaling factor。为了不让点积越来越大，通过除以维度的开方来降低 softmax 的差距。Multi-Head Attention通过使用多个 self-attentions 机制来获取子空间更多方面的表达，用 concat 把多个 self-attentions 的输出集合起来再用一个 linear transformer 转换到原来的维度，如下图。在论文中使用了 8 层 attention layer。Multi-Head AttentionApplications of Attention in Model 在 encoder-decoder attention 层，query 是来自 decoder 上一层的，value 和 key 是来自 encoder 的输出的，这使得解码器中的每个位置都能关注到输入序列中的所有位置。 encoder 的 self-attention 中，query, key, value 都是来自于上一层的输出，编码器中的每个位置都可以关注到编码器前一层的所有位置。 同样，decoder 中的自我关注层允许解码器中的每个位置关注已经预测的位置，防止观察到待预测值Position-wise Feed-Forward Networksfully connected feed-forward network 包含了两个 linear transformer 和 ReLU 激活函数在中间FFNEmbeddings and Softmax与其他序列转换模型类似，使用学习到的 emebddings 来转换输入的 tokens 和输出tokens 转换为维度为 $d_{model}$ 的向量。还使用 linear transformation 和 softmax 函数将 decoder 的输出转换为预测的下一个 token 的概率。在模型中，在两个 embedding 层和 pre-softmax linear transformation 之间共享相同的权重矩阵。在 embedding 层，会将其乘以权重 $\\sqrt {d_{model}}$ 。Positional Encoding为了保持序列顺序信息，必须添加 position encoding 信息到 input embeddings 中，为了可以相加，position encoding 和 embedding 维度相同。在论文中，使用的是公式：Positional EncodingWhy Self-Attention 每一层的计算复杂度 并行能力 解决序列长距离依赖问题Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours." }, { "title": "XGBoost: A Scalable Tree Boosting System", "url": "/posts/xgboost-a-scalable-tree-boosting-system/", "categories": "papers", "tags": "XGBoost, Model Ensemble", "date": "2022-03-27 08:46:00 +0800", "snippet": "Paper 阅读笔记XGBoost: A Scalable Tree Boosting System之前写 ensemble 那篇笔记的时候碰上了这些 boosting 的方法，看了几个讲解 XGBoost 的视频后还是不太了解，于是开始阅读这篇 paper，读了差不多七八个小时才读完…. 看完后感觉基本的概念都了解一点，需要阅读或者实际使用一下这个系统才能更好的体会到各个技术点的作用。Contributions: We design and build a highly scalable end-to-end tree boosting system. We propose a theoretically justified weighted quantile sketch for efficient proposal calculation. We introduce a novel sparsity-aware algorithm for parallel tree learning. We propose an effective cache-aware block structure for out-of-core tree learning. Tree Boosting正则化的目标函数 Regularized Learning Objective预测值是所有树的预测之和在给定的数据集 $\\mathcal{D}$ 中，有 $n$ 条数据 $m$ 个特征，Tree Boosting 中有 $K$ 棵回归树 (CART树)，每个回归树的叶子结点都有数值。对于每个输入 $x$，可以使用 $q$ （树结构）计算出所对应的叶子结点，叶子结点的值 $w$ 即为预测结果 $\\hat y$。而在整个 ensemble 模型中，所有树的预测结果相加就是最终结果 $\\hat y$，如式 (1) 所示。这里可以理解为每一棵树都是为了弥补前一棵树的不足（残差）而训练：$y_{i} = y - y_{i-1}$，这也意味着在 boosting 没有办法像 bagging 的 ensemble 方法一样并行的训练所有树。预测值得到预测值之后，怎样去训练模型使得预测值和真实值更接近呢？通过 minimize 以下目标函数训练模型，目标函数由两部分组成： 第一部分 $l$ 是损失函数 loss function，用于衡量预测值和实际值的差距，这个是可以自定的，只要可以一阶二阶可求导的凸函数就好。 第二项 $\\Omega$ 是惩罚项/正则项 penalty，用于限制模型复杂程度，防止过拟合。其中 $T$ 是叶子结点个数，$\\omega$ 是叶子结点值的和， $\\gamma$ 和 $\\lambda$ 是系数，可以自己调整权重。目标函数Gradient Tree Boosting因为式 (2) 中有函数作为参数，所以很难使用传统的方法去优化这个函数。以下式子是在第 $t$ 棵树中的目标函数，其中 $ \\hat {y} _ {i} ^ {(t-1)}$ 是第 $i$ 条数据在第 $t-1$ 棵树中的预测值。这里就是式 (2) 中 $\\hat y_{i} = \\hat y_{i}^{t-1}+f_{t}(x_{i})$ ，也就是补残差。目标函数然后使用二阶泰勒展开/牛顿法去近似表达这个目标函数，方便后续更高效的优化目标函数，得到新的目标函数泰勒展开式近似的目标函数其中就是把 $\\hat y_{i}^{t-1}$ 视为泰勒式中的 $a$，把 $f_{t}(x_{i})$ 视为 $(x-a)$ ，$g_{i}$ 是目标函数对上一棵树的预测值 $\\hat y_{i}^{t-1}$ 的一阶导数 ${f}’(a)$，$h_{i}$ 为二阶导数 ${f}’’ (a)$。移除常数项 $l(y _ {i} , \\hat y ^ {t-1})$ 之后获得以下第 $t$ 棵树的目标函数式 (3)目标函数接着，定义 $I_{j} = {i \\mid q\\left(\\mathbf{x}_{i}\\right) = j}$ 为叶子结点 $j$ 的数据集合，将正则项 $\\Omega$ 代入到目标函数中正则项将用数据求和改成用叶子结点的数据集合求和，即 \\(\\sum_{i=1}^{n}g_{i} f_{t}\\left(\\mathbf{x}_{i}\\right)=\\sum_{j=1}^{T}\\left(\\sum_{i \\in I_{j}} g_{i}\\right) w_{j}\\) ，合并同类项得到式 (4)目标函数接着，在式 (4) 中把 $w_{j}$ 视为变量，目标函数即为一元二次方程，取 $-\\frac{b}{2a} $ 时为最优解，所以求得叶子结点 $j$ 的最优值 $w_{j}^{*}$最优叶子结点值将最优解代入式 (4) 中，得到最优目标函数值；这个式子可以用来衡量树结构 $q$ 的优劣，利用这个式子就可以像决策树的信息增益一样进行特征选择，进而优化树的结构。目标函数如何判断分割的效果呢？每次树进行分割时，假定左边数据集为 $I_{L}$ 和右边数据集为 $I_{R}$ ，当前结点的总数据集 $I=I_{L}+I_{R}$ ，使用以下式子去计算切分后目标函数减小的数值：左子树分数与右子树分数的和减去不分割情况下的分数以及加入新叶子节点引入的复杂度代价。 $\\gamma$ 是抑制节点个数的，是节点分裂的阈值；$\\lambda$ 是抑制节点值不要太大。分割后目标函数减小值Shrinkage and Column Subsampling除了添加正则项外，还增加了两个额外的技术去防止过拟合。 第一个是 shrinkage，shrinkage 会给每一个新增的叶子结点数值乘以 $\\eta$ ，相当于优化器的学习率，它降低了每棵树的叶子结点对将来的树的影响。降低每棵树对模型的优化程度，利用更多的树慢慢的逼近结果，使得学习更加平缓，可以更好的避免过拟合。 第二个是 column (feature) subsampling，根据用户反馈，使用 column subsampling 可以比传统的 row subsampling 更加有效的防止过拟合，同时也加快了并行计算的速度。和随机森林的应用是一样的，支持列抽样可以降低过拟合，同时减少了计算量。Split Finding AlgorithmsBasic Exact Greedy Algorithm以上介绍了 boosting 方法，但是如何根据式 (7) 找到最好的分裂树的方法还是最大的问题。如果是在分裂时将所有的特征都计算一遍 $L_{\\text {split }}$ 则称为 exact greedy algorithm 。如 Algorithm 1 所示，它遍历了所有可能的特征，根据式 (7) 找出最大的 $score$ ，作为其最优分裂方案，为了提升效率，它必须先对叶子结点按照输入值排序，再去累加 $G_{L}$ 和 $H_{L}$，这样效率太低了。Exact Greedy AlgorithmApproximate AlgorithmExact Greedy Algorithm 很强但当数据不能全部存入内存时很难高效地执行，而且在分布式计算中也会有问题。因此，Approximate Algorithm 应运而生。Approximate Algorithm第一步根据特征分布选出候选分裂点 (candidate splitting points)，然后将特征根据候选分裂点进行 split，再累加各个区域的特征的 $g_{j}$ 和 $h_{j}$，根据式 (7) 算出分裂效果，然后从这些候选分裂点中找出最佳方案。而候选分裂点密度越高时就越接近 Exact Greedy Algorithm，准确率也就越高；相反，密度越低，计算的越快，拟合程度低，防止过拟合就越好。选取候选分裂点有两种方案。global variant 在生成树结构之前就提出了所有的 candidate splits，后续不更新。local variant 会在每次分裂结束后重新提出新的候选方案。Global 的方法可以减少提出候选方案的次数，然而每次的候选数量会比 local 的多。而 local 则是每次分裂结束后都重新选候选方案，对于更深的树，可以选出更加合适的 candidates。以下实验也说明了，在 global 给出足够多的 candidates 时，准确率也可以和 local 的一样。两种方案对比Weighted Quantile Sketch那么如果挑选候选分裂点呢？通常根据 feature 的值均匀的分裂。首先 $D_{k}={(x_{1 k}, h_{1}),(x_{2 k}, h_{2}) \\cdots(x_{n k}, h_{n})}$ 表示第 $k$ 个 feature 的所有数据的值和二阶导数，然后定义 rank functions 式 (8)，表示第 $k$ 个特征中值小于 $z$ 的数据的比例，由 $h$ 加权。目的就是找到所有的 $z$ 使得每一个区间的比例约等于 $1/\\varepsilon $ ，这个 $\\varepsilon $ 是一个分裂密度的参数，值越大，表示区间越小，越接近 exact greedy algorithm。候选分裂点选取为什么要用 $h$ 加权呢？这里将式 (3) 转化为以下式子，这里就是一元二次方程转为完全平方式。转换后的目标函数正是对 $g_{i}/h_{i}$ 的加权方差 (weighted squared loss)，权为 $h_{i}$ 。目标函数目标函数当每一个数据都有相同的权重时，quantile sketch 可以解决这问题。然而目前没有算法解决加权数据，因此提出了有理论支撑的、可以解决加权数据的 a novel distributed eighted quantile sketch algorithm ，大概思路就是一个数据结构支持 merge 和 prune 操作，每个操作都保持一定的准确率，具体的描述和细节在论文附录中。Sparsity-aware Split Finding在实际问题中，很多输入是 sparse 的，有以下几个原因：数据缺失值、很多零值、特征工程的特性（onehot）。因此提出了为每个树节点增加默认的方向 default direction，如下图。default direction当一个值缺失时，会划分到默认的方向。最优的默认方向是从数据中学习而来，如下图 Algorithm 3，$x_{ik}$ 是 no-missing 的数据，只对这些不缺失的数据进行 accumulate，根据这些数据进行划分，不管 missing 数据。sparsity-aware split finding实验证明，处理了这些 sparse 数据之后，快了 50 倍，证明了这个处理是有必要的。sparse实验System DesignColumn Block for Parallel Learning树结构优化最耗时的部分就是数据排序，提出了将数据存在一种 in-memory units, block，数据在每一个 block 中是按照 compressed column (CSC) 格式存储的，每一个 column 按照对应的特征值进行排序，该排序能复用。在 exact greedy algorithm 中，把整个数据集存到一个 blcok 中，然后二分搜索已经提前完成排序的 entries，一次遍历就可以收集到所有叶分支的 split candidates 的 gradient 信息。下图展示了如何将数据集传到 CSC 格式，以及利用 block 结构找到最优分解:block structure for parallel learning这个 block 结构也帮助 approximate algorithms。使用多个 blocks，每个 block 对应着数据子集，不同 block 可以分布在不同的机器上。使用这种已经完成排序的结构，quantile finding step 线性的遍历排好序的 columns。这个对 local proposal algorithms 很有帮助，因为 candidates 很频繁的生成。时间复杂度分析：$d$ 为树的最大深度，$K$ 为树的个数。 对于 exact greedy algorithm，原始的 sparse aware algorithm 的时间复杂度是 \\(O\\left(K d\\|\\mathbf{x}\\|_{0} \\log n\\right)\\) ，这里用 \\(\\|\\mathbf{x}\\|_{0}\\) 表示非缺失值的个数。在 block structure 上 tree boosting 时间复杂度为 \\(O\\left(K d\\|\\mathbf{x}\\|_{0}+\\|\\mathbf{x}\\|_{0} \\log n\\right)\\) 这里的 \\(\\|\\mathbf{x}\\|_{0} \\log n\\) 就是 block structure 只计算一遍排序的时间。当需要多次排序时 block strcuture 可以节省很多计算。 对于 approximate algorithm 来说，用二分搜索原始的时间复杂度是 \\(O\\left(K d\\|\\mathbf{x}\\|_{0} \\log q\\right)\\) ，这里的 log 参数 $q$ 通常是在32到100之间；使用 block structure 之后，可以把时间降到 \\(O\\left(K d\\|\\mathbf{x}\\|_{0}+\\|\\mathbf{x}\\|_{0} \\log B\\right)\\) ，这里也同样的是把只排序一遍。Cache-aware Accessblock structure 对搜索有帮助，但对于 gradient statistics 来说还是需要按行来读取。非连续内存的读写会拖慢 split finding 的速度。对于 exact greedy algorithm 来说，可以通过 cache-aware prefetching algorithm 缓解这个问题；具体而言，给每个线程分配了连续的 buffer 用于存储 gradient statistics，使其可以在连续内存中读取信息，然后用 mini-batch 方法进行 accumulation，在数据较多的情况下减少运行时间。如图 7 显示，大数据集情况下运行时间快了两倍。Cache-aware Access对于 approximate algorithms，通过选择正确的 block size 去解决，这里的 block size 是指每个 block 最多可以存储数据的个数。选择过小的 block size 会减小每个线程的负担导致不能高效的并行运算。过大的 block size 会导致缓存不够，命中率低。经过对比几次实验后，发现每个 block 存储 $2^{16}$ 个 examples 可以很好的平衡 cache property 和 parallelizattion。Blocks for Out-of-core Computation如何将一个机器的所有资源都充分利用呢？除了处理器和内存外，还可以使用磁盘空间去处理不在主内存中的数据。为了实现 out-of-core computation ，把数据分为多个 blocks 存到磁盘中，在计算的同时，使用单独的线程去 pre-fetch 这些 block 到内存中，因此 computation 可以在并行读取 disk 中发生。然而，这也不能完全解决问题，disk 的读取会耗费很长的 computation time，要想办法减小开销和增加 disk IO 的吞吐量，主要用了两个技术提升 out-of-core-computation。 Block Compression：block 按列压缩，并由独立的线程在加载到主内存时解压缩。对于 row index，只记录对于 beginning index 的偏移量，用 16 bit integer 存储。这里每一个 block 可以存储 $2^{16}$ 个 examples，压缩率达到 26% 到 29%。 Block Sharding：将数据拆分到多个磁盘中，为每个磁盘分配一个 prefetch 线程，将数据取到内存缓冲区中。然后，线程交替地从每个缓冲区读取数据。当有多个磁盘可用时，这有助于提高磁盘读取的吞吐量。Related Works论文中按照各个技术列了一下对应的相关工作… Gradient boosting: additive optimization in functional space Regularized model: prevent overfitting Column sampling: prevent overfitting Sparsity-aware learning: handle all kinds of sparsity patterns Parallel tree learning Cache-aware learning Out-of-core computation Weighted quantile sketch: finding quantiles on weighted data related works End to End Evaluation这一部分就是用 XGBoost 做实验了Conclusion In this paper, we described the lessons we learnt when building XGBoost, a scalable tree boosting system that is widely used by data scientists and provides state-of-the-art results on many problems. We proposed a novel sparsity aware algorithm for handling sparse data and a theoretically justified weighted quantile sketch for approximate learning. Our experience shows that cache access patterns, data compression and sharding are essential elements for building a scalable end-to-end system for tree boosting. These lessons can be applied to other machine learning systems as well. By combining these insights, XGBoost is able to solve real world scale problems using a minimal amount of resources.读后笔记 XGBoost 支持了多类型分类器，传统的GBDT采用CART作为基学习器 XGBoost 支持自定目标函数，只要一阶二阶可导 XGBoost 增加了正则项：叶子结点数量、叶子结点值的和。 shrinkage 降低每棵树的提升效果，相当于学习率 column subsampling 列抽样，防止过拟合，加快运算 对所有 sparsity patterns 可以计算 default direction 基于特征的并行运算，在树进行分裂时，并行计算各个特征的增益 近似分裂算法：在树进行分裂时，可以不去遍历所有情况，而是采用近似划分的方法计算 数据排序实现计算并存储于 block 中，加快效率，方便并行运算 cache-aware 和 Out-of-core computation，利用 cache 和 disk 进行加速 做了很多工程上的优化" }, { "title": "Ensemble", "url": "/posts/ensemble/", "categories": "Technical", "tags": "Model Ensemble, Gradient Boost, XGBoost", "date": "2022-03-26 23:15:00 +0800", "snippet": "准备开始 Kaggle，因此学习梳理一下 ensemble 的方法。Keywords: Model ensemble, Adaboost, Gradient Boost, Random Forest参考资料： ML Lecture 22: Ensemble，集成学习算法总结—-Boosting和Bagging, XGBoost的技术剖析, 从决策树到XGBoostEnsemble 方法梳理概述 Ensemble 就是针对一个任务，训练多个模型，最后把各个学习结果加权输出。 而实际上ensemble的主要技巧就是在保持模型相对精度的情况下，如何增加个体之间的差异度。 多个弱分类器的组合，一般有简单多数投票、权重投票，贝叶斯投票，基于D-S证据理论的整合，基于不同的特征子集的整合。 一般采用弱分类器的原因在于将误差进行均衡，因为一旦某个分类器太强了就会造成后面的结果受其影响太大，严重的会导致后面的分类器无法进行分类。常用的弱分类器可以采用误差率小于0.5的，比如说逻辑回归、SVM、神经网络。 sklearn: AdaBoostClassifier, AdaBoostRegressor, RandomForestClassifier, GradientBoostingClassifier两个类别 Bagging (Bootstrap AGgregation) 例如：随机森林 采用的是随机有放回的选择训练数据（bootstrap sampling）然后构造分类器，最后组合；不容易 overfitting。 子模型是 overfitting 的，训练 models 是无序的（并行）。 Boosting 例如：Adaboost, Gradient Boost Decision Trees (GBDT), XGBoost Boosting 是一种迭代算法，根据上一次迭代的预测结果进行优化（Adaboost 是调整样本权重，XGBoost是补充残差），从而减小误差，降低模型的bias。是基于残差的训练。 子模型是 underfitting 的，训练 models 是有顺序的。 Guarantee: if your ML algorithm can produce classifier with error rate smaller than 50% on training data, you can obtain 0% error rate classifier after boosting. 获取不同的数据集 Re-sampling 用不同的数据子集 Re-weighting 修改数据的权重，改 loss function 即可ModelsAdaboost Idea: training $f_{2}(x)$ on the new training set (re-weighted set) that fails $f_{1}(x)$ 先将 $f_{1}$ 训练一遍，得到 error rate，再根据他的 prediciton 进行数据 weighting，使其 error rate 等于 0.5，再用 $f_{2}$ 在 weighted training set 上训练，这时 $f_{2}$ 就相当于补了 $f_{1}$ 的缺点。 Re-weighting 方法，分类错的数据 weight 增加，分类对的数据 weight 减小 上图 d1 通过计算得到值，使其 reweight 之后 error rate 恰好等于 0.5：分类正确的权重和等于分类错误的权重和，下图 $\\varepsilon_{1} $ 是 $f_{1}$ 分类错误的数据权重除以总权重。 How to aggregate all classifier? 错误率低的 classifier 有较大的 $\\alpha$ ，错误率低的 classifier 在 aggregate 中有较高权重。 即使 training data 已经 fit 了，不断增加 classifier 会继续增加 margin，使 model 更加 robust Gradient Boosting Gradient Boosting 的主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向，使得新的 Loss 最小。 可以定任意的 objective function，如下图的 $l\\left(\\hat{y}_{0}^{n}, g\\left(x^{n}\\right)\\right)$，就是 general 的 adaboost。 新的模型 $f_{t}(x)$ 是和之前所有模型的集成模型 $g_{t-1}(x)$ 是互补的，从而得到新的更好的集成模型 $g_{t}(x)$ 。怎么判断 $g_{t}(x)$ 是不是优秀呢，需要定义一个 objective function。 这里推导可以参考 ML Lecture 22: Ensemble 1:23:00 左右 XGBoost (eXtremeGradient Boosting) CART 分类回归树（classification and regression tree）是 xgboost 的基础模型，使用基尼系数计算增益进行特征选取，基尼系数代表着模型的不纯度，基尼系数越小，不纯度越低，特征越好，和信息增益率相反。 其中， $p(k)$ 是分类 $k$ 出现的概率，K 是分类的数目。Gini(D) 反映了从数据集 D 中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D)越小，则数据集D的纯度越高。 additive training 叠加式训练，预测结果由 K 棵树结果相加，即第 $k$ 棵树的结果应该是第 $k-1$ 棵树的结果与真实值的差。 目标函数 Objective function = loss + 惩罚项 penalty/regularization，惩罚项是为了降低模型复杂度，防止过拟合，这里就是控制每棵树的复杂度（叶节点个数/树的深度/叶节点值） 用 泰勒级数 近似目标函数，从而简化目标函数 树的复杂度，第一项是叶的节点数，$\\gamma$ 控制权重，第二项是叶的值，$\\lambda$ 控制权重。在选择树的形状时，通过树的复杂度的计算来选择特征分裂，从而获得复杂度最低的树。 Random Forest (随机森林)Random Forest: Bagging of decision tree, 利用 bootstrap sampling 取样本训练多个决策树，最终结果由所有决策树投票得出 优点：可以处理高维数据、可以判断特征重要性以及特征之间的联系、不容易过拟合、可以处理不平衡数据集 缺点：在噪音较大的分类或回归问题上会过拟合、比决策树计算成本更高 随机森林有效缓解 overfitting 主要依靠了其中三个随机过程，即产生决策树的样本是随机生成，构建决策树的特征值是随机选取，树产生过程中裂变的时候是选择N个最佳方向中的随机一个裂变的。当随机森林产生的树的数目趋近无穷的时候，理论上根据大数定理可以证明训练误差与测试误差是收敛到一起的。link 对于单个决策树模型，每次分裂时根据信息增益/信息增益比/基尼指数选择最好的特征进行分裂 bootstrap sampling：有放回的采样，会导致约有36%的样本永远不会被采样到。 假设有m个样本，有放回的采样，每次被采样到的概率是(1/m),每次不被采样到的概率是(1-1/m)；则(1-1/m)的n次方，当n足够大是，极限为1/e（约等于36%）。 out-of-bag (oob)：根据 bootstrap sampling，使用没被训练过的 36%的样本作为验证集，求得的误差就是 out-of-bag error from sklearn.ensemble import RandomForestClassifierclf = RandomForestClassifier(random_state=0, oob_score=True)clf.fit(x_data, y_data)print(clf.oob_score_)" }, { "title": "Tools", "url": "/posts/Tools/", "categories": "Common", "tags": "tools", "date": "2022-03-20 22:54:00 +0800", "snippet": "记录一下好用的工具 Latex 公式编辑器 坐标轴画图 tinypng 翻译狗 文档 PyTerrier PyTorch " }, { "title": "Graph Convolutional Networks for Text Classification", "url": "/posts/graph-convolutional-networks-for-text-classification/", "categories": "papers", "tags": "NLP, GCN", "date": "2022-03-20 22:15:00 +0800", "snippet": "Paper 阅读笔记Graph Convolutional Netowrks for Text Classification构建 双层 GCN 异构图，words 和 documents 做节点，words 之间的 edge 用词共现信息表示，word 和 documents 之间的 edge 用 tf-idf 表示。然后做把文本分类转换为点分类，该方法可以在小比例的标记文档中实现强大的分类性能，可以同时学习单词和文档节点 embedding。源码 在这里。Keywords: NLP, Text GCN, GCN, Text Classification, GraphContributions： 提出了一个用于文本分类新的图神经网络方法，可用于联合学习 word 和 document embeddings 几个基准数据集显示这个方法 state-of-the-art，没有用到预训练词向量和额外知识。Related Work： Deep learning text classification studies can be categorized into two groups. Models based on word embeddings Model employed deep neural networksMethod：GCN graph $ G = (V, E)$ where $V, E$ are sets of nodes and edges $ (v, v) \\in E $ for any $v$, every node is assumed to be connected to itself $X \\in \\mathbb{R}^{n \\times m}$ 是特征矩阵，n 个节点 m 维特征；$X_{v} \\in \\mathbb{R}^{m}$ 表示 $v$ 的特征向量 $A$ 是 $G$ 的邻接矩阵，度矩阵 $D$ where $D_{i i}=\\sum_{j} A_{i j}$ 就是邻接矩阵求每个点的度 (邻接数)；$A$ 的主对角线都设为 1 因为每个点都连接自己 单层 GCN 时每个点都只能获取距离为一的邻接点的信息，而多层 GCN 叠加的时候就可以把更广的范围信息整合起来 单层 GCN，新的 $k$-dimensional node 的特征矩阵 $L^{(1)} \\in \\mathbb{R}^{n \\times k}$ 可以被计算为 $L^{(1)}=\\rho\\left(\\tilde{A} X W_{0}\\right)$ (1) :+1::+1: where $\\tilde{A}=D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$ is normalized symmetric adjacency matrix and $W_{0} \\in \\mathbb{R}^{m \\times k}$ is a weight matrix, $\\rho$ 是激活函数 多层 GCN： $L^{(j+1)}=\\rho\\left(\\tilde{A} L^{(j)} W_{j}\\right)$ (2) where $j$ denotes the layer number and $L^{(0)} = X$ Text GCN 建立异构图可以把词共现信息显性的表现在graph中，方便图卷积 图的点数量 V 等于 documents 数 + words 数 (corpus size + vocabulary size) 输入时简单设置特征矩阵 $X = I$ ，每个word/document都表示为 one-hot vector Document node 和 word node 的边权重为 word 在 document 中的 TF-IDF，他们测试过 TF-IDF weight 比只用 term frequency 的要好 使用 point-wise mutual information (PMI) 计算两个 word node edge 的权重，PMI 要比共现次数效果好 综上得到邻接矩阵 $A$ PMI 值计算 word node $i, j$ where #W(i) 是 sliding windows 的包含 word $i$ 的数量 #W(i, j) 是包含 word $i, j$ 的 sliding windows 数量 #W 是 sliding windows 在 corpus 的总数量 一个正的 PMI 值说明语义上两个词强关联，反之弱关联；因此只把 PMI 为正的边添加到 graph 中（threshold -&amp;gt; sparse） 建立好 graph 之后把它放进两层 GCN 中：第一层根据公式 (1) 算出新的特征向量，再扔进第二层公式(1) 又算出新的特征向量，然后扔进 softmax 里生成等同标签数的概率，进行分类 $Z=\\operatorname{softmax}\\left(\\tilde{A} \\operatorname{ReLU}\\left(\\tilde{A} X W_{0}\\right) W_{1}\\right)$ （7） loss function 就用的 cross-entropy，只计算了 labeled document 节点: (8) where $\\mathcal{Y}_{D}$ is the set of labeled document indices $F$ is the dimension of the output features $Y$ is the label indicator matrix (label 矩阵) 公式 (7) 的 $W_{0}, W_{1}$ 可以用 gradient descent 训练，示意图如下 Schematic of Text GCN 两层 GCN 最多允许 距离为 2 的信息传递，即使没有 document 之间的连边，两层 GCn 也允许 documents 之间传递信息。他们实验说两层的要比一层的要好，更多层也没有提升效果。 Experiments：Determine: Can our model achieve satisfactory results in text classification, even with limited labeled data? 能不能使用少量的 labeled data 训练得到不错的结果，在文本分类中 Can our model learn predictive word and document embeddings?Baselines TF-IDF + LR：tf-idf 特征 + Logistic Regression classifier CNN CNN-rand：随机初始化embedding CNN-non-static：无监督学习的 embedding，训练中 fine tuned LSTM：有/无 预训练词向量 Bi-LSTM：预训练词向量 PV-DBOW：BOW，Logistic Regression Classifier PTE：词向量 average 做 doc向量 fastText：word/n-grams 向量均值做 doc向量 + linear classifier SWEM：词向量 做 simple pooling strategies LEAM：词和标签在同一空间进行分类，用到了 label description Graph-CNN-C：graph CNN + shev filter Graph-CNN-S：graph CNN + Spline filter Graph-CNN-F：graph CNN + Fourier filterDatasets用了五个数据集：20-Newsgroups (20NG), Ohsumed, R52 and R8 of Reuters 21578 and Movie Review (MR).Preprocess Cleaning &amp;amp; Tokenizing Removed stop words NLTK, low frequency words less than 5 times for some datasets. MR 数据集太小了就没移除低频词 Summary statistics of datasets Settings Text GCN, embedding size of first convolution as 200, window size as 20, learning rate 0.02, dropout rate 0.5, L2 loss 0. 10 fold Cross-validation, 200 epochs, Adam optimizer, early stopping (10 epoch) Baseline model with default parameter setting as original papers Baseline pretrain word embeddings: 300-dimensional GloVePerformanceTest Accuracy Text GCN performs the best and significantly outperforms all baseline models on four datasets except MR. 说明长文本数据集表现不错 Word nodes can gather comprehensive document label information and act as bridges or key paths in the graph, so that label information can be propagated to the entire graph. 这里作者说的很棒，document 的标签信息影响到了词向量的表达 However, we also observed that Text GCN did not outperform CNN and LSTM-based models on MR. This is because GCN ignores word orders that are very useful in sentiment classification, while CNN and LSTM model consecutive word sequences explicitly. 总结了在 MR 数据集上表现不如 CNN 和 LSTM 的原因，没有利用到语序，结果在情感分类表现就没有序列模型那么好了。 Another reason is that the edges in MR text graph are fewer than other text graphs, which limits the message passing among the nodes. There are only few document-word edges because the documents are very short. The number of word-word edges is also limited due to the small number of sliding windows. 再有就是在短文本 corpus 里，document 连接的 word node 就很少，导致标签信息很难传开，这也和 sliding windows 大小有关。Parameter Sensitivity 作者测试了 不同 sliding windows 大小对模型的影响，实验表明过大和过小都会有负影响。说明了太小的不能生成足够的词共现信息，太大的导致不关联词语也连接上边，减少了差异性 也测试了第一层 GCN 不同维度的 embeddings，太小的维度导致 标签信息无法传开；太大的维度导致训练成本增加，同时性能也没提升。Effects of the Size of Labels Data 在 20NG 和 R8 数据集上做了几个模型在不同比例 labeled data 的数据集情况下的 accuracy 分析。结果说明了 GCN 在低标签数据集中也可以有很好的表现 20% training documentsDocument Embeddings t-SNE 分析了生成的 document embeddings，和其他几个模型做了对比。说是 Text GCN 可以学习到更有区分度的 embeddings document embeddings Word Embeddings 同样用 t-SNE 对比 Text GCN 所生成的 word embeddings 和其他模型。Discussion From experimental results, we can see the proposed Text GCN can achieve strong text classification re- sults and learn predictive document and word embeddings. However, a major limitation of this study is that the GCN model is inherently transductive, in which test document nodes (without labels) are included in GCN training. Thus Text GCN could not quickly generate embeddings and make prediction for unseen test documents. Possible solutions to the problem are introducing inductive (Hamilton, Ying, and Leskovec 2017) or fast GCN model (Chen, Ma, and Xiao 2018). 作者所提出的 Text GCN 可以达到很好的文本分类结果 主要的问题是 GCN 本质上是传导的模型，测试数据是被包括在训练中的，因此 Text GCN 不能很快的生成 embeddings 以及作出预测 可能的解决方案是 引入 inductive or fast GCn modelConclusion nad Future Work In this study, we propose a novel text classification method termed Text Graph Convolutional Networks (Text GCN). We build a heterogeneous word document graph for a whole corpus and turn document classification into a node classification problem. Text GCN can capture global word co-occurrence information and utilize limited labeled documents well. A simple two-layer Text GCN demonstrates promising results by outperforming numerous state-of-the- art methods on multiple benchmark datasets. In addition to generalizing Text GCN model to inductive settings, some interesting future directions include improving the classification performance using attention mechanisms (Velicˇkovic ́ et al. 2018) and developing unsupervised text GCN framework for representation learning on large scale unlabeled text data.个人总结： 本文的方法只用了词词连边、词文连边，词和词之间都是有连边的，相当于把 document 放进了 word 的 graph 里，也就是用 word embedding 表达 doc embedding，本质上是 Bag-of-words model；多了 document 之间信息的传递，把 word2vec 和 doc2vec 两个步骤合在了一起，使得 word 和 document 的 embedding 相互影响。也正如 contribution 所说，可以不利用预训练的词向量和预先知识，同时计算 word 和 document 的 embedding。 Text GCN 出现新的 document 时会影响到 word 和 word 之间权重，所以需要重新训练调整边的权重，不能很快的得到新 document 的 embedding 的分类结果。 闲谈： 曾几何时我本科毕业论文题目也是 异构图神经网络判别推特水军 … 不过我没用到文本信息，只是从社交网络 graph 来判别… 读这篇 paper 的初衷是完成 Text as Data 这门课的 coursework… 有空把剩下几篇也做一下笔记 Class-Based n-gram Models of Natural Language Enriching Word Vectors with Subword Information GloVe-Global Vectors for Word Representation Graph Convolutional Networks for Text Classification Latent Dirichlet Allocation Translating Embeddings for Modeling Multi-relational Data Word representations- A simple and general method for semi-supervised learning " }, { "title": "Gitpage Blog Setup", "url": "/posts/gitpage-blog-setup/", "categories": "Common", "tags": "tools, script", "date": "2022-03-19 23:30:00 +0800", "snippet": "这一篇记录一下整个建立 github-page blog 的流程，以及针对个人习惯做了怎么样的改动。Keywords: mac m1, GitHub Pages, jekyll-theme-chirpy, jekyll-compose, Google Analytics参考链接：GitHub Pages 搭建教程, Chirpy Getting Start,搭建初衷最近在忙着做学校的 coursework，以及准备工作面试，无意中在知乎刷到一篇关于搭建个人技术博客的文章，就想着搭一个试试看（种树/十年前/现在）。GitHub Pages新建 Github Pages 仓库 首先需要找一份自己喜欢的 模版，本文使用 Chirpy，根据 指导 从 Chirpy Starter fork 一个新仓库，取名为&amp;lt;GH_USERNAME&amp;gt;.github.io，GH_USERNAME 是 Github 用户名本地环境配置 本地安装 ruby，检查 路径 和 版本，安装 jekyll，检查版本 $ brew install ruby$ which ruby &amp;amp;&amp;amp; ruby -v$ gem install jekyll bundler$ jekyll -v 把刚刚 fork 的 repo 拉到本地 git clone xxx.git，安装依赖 $ bundle 配置个人信息，在 _config.yml 文件中 配置 title, tagline, 以及其他你想要展示的社交平台信息(twitter, github, email)，还有很多客制化的东西，具体看文件内注释，这一步不影响 本地运行预览，启动服务器后，浏览器打开 http://127.0.0.1:4000 即可看到效果 $ bundle exec jekyll s 部署到 github 上，首先检查以下文件是不是都在目录里，完成后就可以 commit 和 push 上去了 .github/workflows/pages-deploy.yml，如果没有的话，创建一个新的，把 sample file 拷贝进去，on.push.branches 的值要和你仓库默认分支名字相同 tools/deploy.sh，没有的话拷贝一份 建议不要上传 Gemfile.lock，在 .gitignore 添加一下忽略扫描就好 发布你的网站 push 之后，等 build 完了，就会出现一个新的分支 gh-pages 用于存储网站文件 打开 Settings -&amp;gt; Pages -&amp;gt; Source 选择 gh-pages/root 作为发布源，点击保存 publishiing source 就可以点开上方链接浏览你的网站了 如果远程部署有问题的话，可以打开 Actions -&amp;gt; Workflows 检查部署问题 流量监控 这里使用的是 Google Analytics 进行流量监控，小博客没有流量，这个也不用在意… 创建 GA 账号，创建 Property，到 Set up Data Stream 的位置输入你的网址URL，创建数据流，获得 Measurement ID 类似 G-V6XXXXXXXX，复制到 _config.yml 文件中 google_analytics: id: &#39;G-V6XXXXXXX&#39; # fill in your Google Analytics ID # Google Analytics pageviews report settings pv: proxy_endpoint: # fill in the Google Analytics superProxy endpoint of Google App Engine cache_path: # the local PV cache data, friendly to visitors from GFW region commit &amp;amp; push &amp;amp; deploy 之后就可以在 Google Analytics 中实时查看你的网址流量了 如果你想更进一步，使得页面显示浏览量的话，可以参考 Chirpy Guide 配置网站图标 网站图标就是浏览器标签页最靠左那个图标 favicons 到 Real Favicon Generator 打开你选好的图片，滑倒最底下 Generate your Favicons and HTML code 生成 favicon 下载解压之后，删除 browserconfig.xml，site.webmanifest 两个文件，把其余的拖到项目目录 assets/img/favicons/ 下，如果没有这个路径，就新建文件夹 commit &amp;amp; push &amp;amp; deploy 之后就可以在浏览器看到网址的新图标了 本地配置Jekyll-Compose 部署 Jekyll-Compose 能让你更方便的写博客，这个是自动添加 Front Matter 的，使你可以更专注于写文章 打开 项目文件夹，在 Gemfile 文件中添加 gem &#39;jekyll-compose&#39;, group: [:jekyll_plugins]，然后终端执行 $ bundle 命令有 draft # Creates a new draft post with the given NAMEpost # Creates a new post with the given NAMEpublish # Moves a draft into the _posts directory and sets the dateunpublish # Moves a post back into the _drafts directorypage # Creates a new page with the given NAMErename # Moves a draft to a given NAME and sets the titlecompose # Creates a new file with the given NAME 具体用法可参照 Jekyll-Compose 写文章快捷键 jekyll-compose 可以在 _config.yml 中添加 auto_open: true，即可使用 bundle exec jekyll post newPostName 直接打开 newPostName.md，但在 mac 上用不了，查看源码 jekyll-compose source code 因为 mac 中直接 输入 typora 是调不出程序的… 这里 editor_name 是调用 环境变量的，所以： $ export JEKYLL_EDITOR=&#39;open -a typora&#39; 上一步完成后还是觉得麻烦，必须在项目目录下打开终端才可以使用；因此 $ alias gitpage=&#39;cd [repo directory] &amp;amp;&amp;amp; bundle exec jekyll&#39; 可以直接跳转到项目目录执行 jekyll-compose 命令，打开编辑器立刻开写！ 至此，可以从命令行直接开写了，但我还有 Alfred，于是… alfred 一开始想集成 jekyll 所有功能，做到后面才发现其他操作不需要在 alfred 快捷完成，反而会影响最重要的创建的快捷性，因此只做了单一功能。 简单的实现就是 Keywords + Run Script，以下是脚本代码 #!/bin/bash cd $project_pathbundle exec $jekyll_path post $1filename=`ls -lt _posts | grep md | head -n 1 | awk &#39;{print $9}&#39;` # &amp;gt;&amp;amp;2 echo $filenameopen -a typora _posts/$filename 图片插入 这里我是将图片直接放在 Github repo/_images/ 下，没有额外的做 cdn； 可以在 _config.yml 的 img_cdn 中设置文件夹路径，把路径中的 tree 改成 raw 就能用了，这样设置之后所有 post 的文件都是相对路径了，方便管理，我这边设置是每个 post 一个文件夹； 在本地编辑的话我选用的是 Typora 编辑器，很容易上手… 可以设置插入图片时复制到文件夹，把复制文件夹设为 repo/_images，然后再在 post 的 md 文件头 设置一行 typora-root-url: &quot;../_images&quot; 表示根路径，这样文档内就可以使用同 github 上的相对路径； 有一个小缺点就是 typora 显示的图片和线上效果不一样，这点后续再看看有没有改进方法。 总结本以为搭博客会很快，结果踩了不少坑，但趁机接触了不少新东西，回顾了很多旧玩意儿。各种脚本比想象中有趣很多，有空了可以再继续深究… git workflow, alfred workflow ( iOS Remote ) 等等。TODO: Alfred Workflows publish/update 脚本，用于快速 commit &amp;amp; push 指定 post 图片清理脚本，用于清理图片文件夹内没被用到的图片 解决 Typora 图片显示与网页不一致的问题 流量监控完善，这个不着急" }, { "title": "Text and Typography", "url": "/posts/text-and-typography/", "categories": "Common", "tags": "typography, tools", "date": "2022-03-19 22:54:00 +0800", "snippet": "这个 post 主要用于记录 markdown 的写法，以及利用第一篇 post 的发布进行练手，熟悉完善整个利用 github page 写 blog 的 workflow。TypographyTitle 2Basic fontlink, bold, italic, delete, high, box, https://juhaoliang1997.github.io/, 1, inline codeemoji: :star2: :monkey:Lists Firstly Secondly ThirdlyUnordered list Chapter Section Paragraph Checklist TODO 1 TODO 2 TODO 2.1 Block Quote This line shows the block quote.Tables Company Contact Country Alfreds Futterkiste Maria Anders Germany Island Trading Helen Bennett UK Magazzini Alimentari Riuniti Giovanni Rovelli Italy Images defaultimage1 Shadowshadow effect (visible in light mode) Left aligned Float to left “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Float to right “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Mathematics\\[\\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6}\\]When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\]$x^2$Code blockCommonThis is a common code snippet, without syntax highlight and line number.Specific LanguagesConsole$ env |grep SHELLSHELL=/usr/local/bin/bashPYENV_SHELL=bashShellif [ $? -ne 0 ]; then echo &quot;The command was not successful.&quot;; #do the needful / exitfi;Specific filename@import &quot;colors/light-typography&quot;, &quot;colors/dark-typography&quot;Reverse Footnote note &amp;#8617; " } ]
