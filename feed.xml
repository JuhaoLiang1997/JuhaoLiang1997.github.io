<feed xmlns="http://www.w3.org/2005/Atom"> <id>/</id><title>JuhaoLiang</title><subtitle></subtitle> <updated>2022-05-16T23:39:38+08:00</updated> <author> <name>JuhaoLiang</name> <uri>/</uri> </author><link rel="self" type="application/atom+xml" href="/feed.xml"/><link rel="alternate" type="text/html" hreflang="en" href="/"/> <generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator> <rights> © 2022 JuhaoLiang </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>关于自监督学习模型的随笔</title><link href="/posts/%E5%85%B3%E4%BA%8E%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9A%8F%E7%AC%94/" rel="alternate" type="text/html" title="关于自监督学习模型的随笔" /><published>2022-05-16T20:38:00+08:00</published> <updated>2022-05-16T20:38:00+08:00</updated> <id>/posts/%E5%85%B3%E4%BA%8E%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9A%8F%E7%AC%94/</id> <content src="/posts/%E5%85%B3%E4%BA%8E%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9A%8F%E7%AC%94/" /> <author> <name>JuhaoLiang</name> </author> <category term="Technical" /> <summary> 关于自监督学习模型的随笔 吃完饭休息一会，看李宏毅老师视频做的随笔 参考资料：惡搞自督導式學習模型 BERT 的三個故事, Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model Overview 视频讲述了 BERT 三种有意思的研究： Cross-lingual Cross-discipline Pre-training with artificial data 第一点是关于多语言 BERT 的跨语言能力，例如不同语言通过向量转换进行的字符级翻译，这个是符合直觉的方法，而且是很有意思的研究。 第二点是关于 BERT 跨学科能力，感觉是从 cross-lingual 进阶而来... </summary> </entry> <entry><title>Semi-supervised</title><link href="/posts/semi-supervised/" rel="alternate" type="text/html" title="Semi-supervised" /><published>2022-05-15T20:32:00+08:00</published> <updated>2022-05-15T20:32:00+08:00</updated> <id>/posts/semi-supervised/</id> <content src="/posts/semi-supervised/" /> <author> <name>JuhaoLiang</name> </author> <category term="Technical" /> <summary> Semi-supervised Learning 简单了解一下 semi-supervised learning。 参考资料：ML Lecture 12: Semi-supervised, A Survey on Deep Semi-supervised Learning Overview 半监督学习（semi-supervised learning）指的是大部分训练数据是未标注的，少部分已标注。其中分为 transductive learning 和 inductive learning，前者表示未标注数据是测试数据，且参与训练过程；后者是指未标注数据不是测试数据，测试数据不参与模型训练。 为什么需要半监督学习呢？因为标注数据工作是很昂贵的，半监督学习可以由部分已知数据推断未标注数据。半监督学习通常会伴随着一些假设 assumptions，而这些 assumption... </summary> </entry> <entry><title>Transfer Learning</title><link href="/posts/transfer-learning/" rel="alternate" type="text/html" title="Transfer Learning" /><published>2022-05-14T20:53:00+08:00</published> <updated>2022-05-14T20:53:00+08:00</updated> <id>/posts/transfer-learning/</id> <content src="/posts/transfer-learning/" /> <author> <name>JuhaoLiang</name> </author> <category term="Technical" /> <summary> Transfer Learning 简单了解一下迁移学习（Transfer Learning）。 参考文献：ML Lecture 19: Transfer Learning, Transfer Learning Overview 根据 target data、source data（是否带标签）区分为四种类型： Model Fine-tuning Target data 和 Source data 都有标签时，可以用 Model fine-tuning 做迁移学习。基本思路就是用 source data 先训练模型，再用 target data 微调模型。 One-shot learning 当 target data 数量少的时候，可以称为 One-shot learning: only a few examples in target domain。 ... </summary> </entry> <entry><title>BERT Model</title><link href="/posts/bert-model/" rel="alternate" type="text/html" title="BERT Model" /><published>2022-04-02T03:29:00+08:00</published> <updated>2022-04-02T03:29:00+08:00</updated> <id>/posts/bert-model/</id> <content src="/posts/bert-model/" /> <author> <name>JuhaoLiang</name> </author> <category term="Technical" /> <summary> BERT Model 基于 Huggingface Transformer实战教程 的 BERT 模型笔记，写的有点杂，回头再整理补充一下。 课程相关 课程目标 《Huggingface Transformers实战教程 》是专门针对HuggingFace开源的transformers库开发的实战教程，适合从事自然语言处理研究的学生、研究人员以及工程师等相关人员的学习与参考，目标是阐释transformers模型以及Bert等预训练模型背后的原理，通俗生动地解释transformers库的如何使用与定制化开发，帮助受众使用当前NLP顶级模型解决实际问题并取得优秀稳定的实践效果。 BERT BERT 框架 BERT整体框架包含 pre-train 和 fine-tune 两个阶段。pre-train 阶段模型是在无标注的标签数据上进行训练，fine-tune 阶段，B... </summary> </entry> <entry><title>Transformer: Attention Is All You Need</title><link href="/posts/transformer-attention-is-all-you-need/" rel="alternate" type="text/html" title="Transformer: Attention Is All You Need" /><published>2022-03-31T21:42:00+08:00</published> <updated>2022-03-31T21:42:00+08:00</updated> <id>/posts/transformer-attention-is-all-you-need/</id> <content src="/posts/transformer-attention-is-all-you-need/" /> <author> <name>JuhaoLiang</name> </author> <category term="papers" /> <summary> Paper 阅读笔记 Attention Is All You Need, 【機器學習2021】Transformer, transformer implementation, 代码笔记, Transformer 面经总结 transformer 主要还是利用了 self-attention 的机制，打破了传统 rnn 基于序列的线性训练方法，增强其并行运算能力，克服长距离依赖问题；但与此同时，局部信息的获取没有 RNN 和 CNN 的强。 个人笔记 Transformer 结构 宏观上来讲， transformer 由两部分组成 encoder + decoder。以文本翻译为例（英译中），encoder 负责英文原句子的意思理解，decoder 负责用 encoder 所理解的整体句子意思逐字的生成中文句子。 Encoder 流程 输入表示向... </summary> </entry> </feed>
