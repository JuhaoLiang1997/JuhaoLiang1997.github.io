<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Transformer: Attention Is All You Need" /><meta property="og:locale" content="en" /><meta name="description" content="Paper 阅读笔记" /><meta property="og:description" content="Paper 阅读笔记" /><link rel="canonical" href="/posts/transformer-attention-is-all-you-need/" /><meta property="og:url" content="/posts/transformer-attention-is-all-you-need/" /><meta property="og:site_name" content="JuhaoLiang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-03-31T21:42:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Transformer: Attention Is All You Need" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-03-31T21:42:00+08:00","datePublished":"2022-03-31T21:42:00+08:00","description":"Paper 阅读笔记","headline":"Transformer: Attention Is All You Need","mainEntityOfPage":{"@type":"WebPage","@id":"/posts/transformer-attention-is-all-you-need/"},"url":"/posts/transformer-attention-is-all-you-need/"}</script><title>Transformer: Attention Is All You Need | JuhaoLiang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JuhaoLiang"><meta name="application-name" content="JuhaoLiang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images/avatar.jpg " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JuhaoLiang</a></div><div class="site-subtitle font-italic">记录日常技术博客</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/JuhaoLiang1997" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['JuhaoLiang1997','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Transformer: Attention Is All You Need</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Transformer: Attention Is All You Need</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/JuhaoLiang1997">JuhaoLiang</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1648734120" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-03-31 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3182 words"> <em>17 min</em> read</span></div></div></div><div class="post-content"><h1 id="paper-阅读笔记">Paper 阅读笔记</h1><p><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>, <a href="https://www.youtube.com/watch?v=n9TlOhRjYoc">【機器學習2021】Transformer</a>, <a href="https://github.com/hyunwoongko/transformer">transformer implementation</a>, <a href="https://github.com/JuhaoLiang1997/transformer/blob/master/Transformer%E4%B8%AA%E4%BA%BA%E7%AC%94%E8%AE%B0.ipynb">代码笔记</a>, <a href="https://enze5088.github.io/content/article-4/#">Transformer 面经总结</a></p><p>transformer 主要还是利用了 self-attention 的机制，打破了传统 rnn 基于序列的线性训练方法，增强其并行运算能力，克服长距离依赖问题；但与此同时，局部信息的获取没有 RNN 和 CNN 的强。</p><hr /><h2 id="个人笔记"><span class="mr-2">个人笔记</span><a href="#个人笔记" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-31-transformer-attention-is-all-you-need/截屏2022-03-31 14.52.59.png" alt="截屏2022-03-31 14.52.59" style="zoom:50%;" data-proofer-ignore> <em>Transformer 结构</em></p><p>宏观上来讲， transformer 由两部分组成 encoder + decoder。以文本翻译为例（英译中），encoder 负责英文原句子的意思理解，decoder 负责用 encoder 所理解的整体句子意思逐字的生成中文句子。</p><h3 id="encoder-流程"><span class="mr-2">Encoder 流程</span><a href="#encoder-流程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ol><li><p>输入表示向量 <code class="language-plaintext highlighter-rouge">inputs (seq_len, d_model)</code>: <code class="language-plaintext highlighter-rouge">seq_len</code> 最大 sequence 长度，<code class="language-plaintext highlighter-rouge">d_model</code> 维特征向量（词向量）</p><li><p>添加 position encoding: <code class="language-plaintext highlighter-rouge">inputs_pos (seq_len, d_model) = inputs (seq_len, d_model) + positional encoding (seq_len, d_model)</code></p><ul><li><p>这里 positional encoding 可以用公式计算出来，或者用可训练的（bert）；</p><li><p>直接相加和 concat 到后面没什么区别，不影响原本的向量表达</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-31-transformer-attention-is-all-you-need/截屏2022-03-13 01.42.28.png" alt="截屏2022-03-13 01.42.28" style="zoom:50%;" data-proofer-ignore> <em>position encoding公式</em></p></ul><li><p>Self Attention的部分，$W_{q}, W_{k}, W_{v}$ 是三个可训练的权重矩阵，用于将 <code class="language-plaintext highlighter-rouge">inputs_pos</code> 转换成 Query, Key, Value，<code class="language-plaintext highlighter-rouge">Query Matrix</code>, <code class="language-plaintext highlighter-rouge">Key Matrix</code>, <code class="language-plaintext highlighter-rouge">Value Matrix</code> 每一行代表一个 token 的 Query, Key, Value</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>QueryMatrix(seq_len, d_k) = inputs_pos(seq_len, d_model) * Wq(d_model, d_k)
KeyMatrix(seq_len, d_k) = inputs_pos(seq_len, d_model) * Wk(d_model, d_k)
ValueMatrix(seq_len, d_v) = inputs_pos(seq_len, d_model) * Wv(d_model, d_v)
</pre></table></code></div></div><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-31-transformer-attention-is-all-you-need/截屏2022-03-31 16.11.52.png" alt="截屏2022-03-31 16.11.52" style="zoom:50%;" data-proofer-ignore> <em>Query, Key, Value Matrix计算</em></p><li><p>求得 Query 和 Key 的点积，表示当前 token 对 所有 tokens 的 attention。如果特征维度 <code class="language-plaintext highlighter-rouge">d_model</code> 很大的话，这里点积结果会很大，需要做 scaling 保持其 variance 为 1，使得 softmax 结果差距不会太大，从而解决<strong>梯度消失</strong>的问题。<a href="https://www.zhihu.com/question/339723385/answer/782509914">参考</a></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>AttentionMatrix(seq_len, seq_len) = QueryMatrix(seq_len, d_k) * KeyMatrix.T(d_k, seq_len) / (d_k)^0.5
</pre></table></code></div></div><li><p>将 attention 按行做 softmax，将 attention 压缩到 0 到 1 之间</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>AttentionMatrix(seq_len, seq_len) = softmax(AttentionMatrix, axis=0)
</pre></table></code></div></div><li><p>利用计算得到的 attention 点乘 value，再相加，得到 outputs</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>layer_outputs(seq_len, d_v) = AttentionMatrix(seq_len, seq_len) * ValueMatrix(seq_len, d_v)
</pre></table></code></div></div><li><p>步骤 3-6 是计算单个 Self-Attention 输出的过程。实际上 Multi-Head Attention 是将步骤 3 中 <code class="language-plaintext highlighter-rouge">QueryMatrix</code>, <code class="language-plaintext highlighter-rouge">KeyMatrix</code>, <code class="language-plaintext highlighter-rouge">ValueMatrix</code> 按照<strong>特征维度</strong> (d_model) 切割成<strong>多个子集</strong>，每个子集的特征维度是 <code class="language-plaintext highlighter-rouge">(d // n_head)</code>。将切割后的 QKV Matrix 进行 步骤456 Self-Attention 计算得到各自的 <code class="language-plaintext highlighter-rouge">layer_outputs</code>，再 <code class="language-plaintext highlighter-rouge">concat</code> 在一起，构成输出 <code class="language-plaintext highlighter-rouge">(seq_len, d_v)</code>，最后加个 <strong>linear transformer</strong> 将 <code class="language-plaintext highlighter-rouge">(seq_len, d_v)</code> 转为 <code class="language-plaintext highlighter-rouge">(seq_len, d_model)</code> 就好。</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>subQueryMatrixs, subKeyMatrixs, subValueMatrixs = split(QueryMatrix), split(KeyMatrix), split(ValueMatrix)
subWeightedValues = softmax(scale(subQueryMatrixs * subKeyMatrixs)) * subValueMatrixs
concated_outputs(seq_len, d_v) = concat([subWeightedValue1, subWeightedValue1, ...])  
transformed_outputs(seq_len, d_model) = concated_outputs(seq_len, d_v) * linear_transformer(d_v, d_model) 
</pre></table></code></div></div><li><p>步骤 7 得到了 Multi-Head Attention 的结果后，需要做一个残差连接，与 <code class="language-plaintext highlighter-rouge">input_pos</code> 进行 <code class="language-plaintext highlighter-rouge">Residual &amp; Norm &amp; Dropout</code> 的操作</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>add_outputs(seq_len, d_model) = transformed_outputs(seq_len, d_model) + inputs_pos(seq_len, d_model)
multihead_outputs(seq_len, d_model) = Dropout(LayerNorm(add_outputs))
</pre></table></code></div></div><ul><li><p><code class="language-plaintext highlighter-rouge">Batch Norm</code> 是对每一个维度进行 normalization，<code class="language-plaintext highlighter-rouge">Layer Norm</code> 是对单个 token 的特征向量进行 normalization，<code class="language-plaintext highlighter-rouge">Layer Norm</code> 会消除同一特征的差异性，一般 <code class="language-plaintext highlighter-rouge">Batch Norm</code> 用于图像，<code class="language-plaintext highlighter-rouge">Layer Norm</code> 用于NLP，而 NLP 的 embedding 一般计算 cos similarity 作为相似度，所以单一特征差异性其实不关键。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-31-transformer-attention-is-all-you-need/截屏2022-03-31 16.41.05.png" alt="截屏2022-03-31 16.41.05" style="zoom:50%;" data-proofer-ignore> <em>Batch Normalization</em></p></ul><li><p>这里到了 <code class="language-plaintext highlighter-rouge">Feed-Forward</code> 的部分了，利用俩全连接层将 <code class="language-plaintext highlighter-rouge">multihead_outputs</code> 提炼到高维空间 (论文中维度提高了四倍)，利用 <code class="language-plaintext highlighter-rouge">ReLU</code> 进行激活，原理和 SVM 差不多，将低维特征映射到高维更容易区别特征差异。上一步加 <code class="language-plaintext highlighter-rouge">Layer Norm</code> 也是为了在这一步激活函数更好的发挥作用</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>latent_represent(seq_len, latent_d) = Dropout(ReLU(multihead_outputs * w1 + b1))
ff_output(seq_len, d_model) = latent_represent * w2 + b2
</pre></table></code></div></div><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-31-transformer-attention-is-all-you-need/截屏2022-03-31 16.47.11.png" alt="截屏2022-03-31 16.47.11" style="zoom:50%;" data-proofer-ignore> <em>Feed-Forward network</em></p><li><p>同步骤 8，给 Feed-Forward Network 再加一个 <code class="language-plaintext highlighter-rouge">Residual &amp; Norm &amp; Dropout</code>，作用一是解决梯度消失的问题，二是解决权重矩阵的退化问题</p><li><p>步骤 3-10 是一个 encoder 部分需要重复的 network，论文中重复了 $N=6$ 次。至此就完成了 encoder 的部分了，也就是 transformer 结构图中左半部分，输出结果是和输入特征维度一致的 <code class="language-plaintext highlighter-rouge">(seq_len, d_model)</code>。</p></ol><h3 id="decoder-流程"><span class="mr-2">Decoder 流程</span><a href="#decoder-流程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ol><li><p>输入 output 的表示向量 <code class="language-plaintext highlighter-rouge">outputs (seq_len, d_model)</code>: seq_len 个 tokens，d_model 维特征向量。</p><ul><li><code class="language-plaintext highlighter-rouge">seq_len</code> 是由最大 sequence 长度决定的，短的 sequence 由 <code class="language-plaintext highlighter-rouge">&lt;mask&gt;</code> 做 padding<li>训练阶段数据就是 target sequence 偏移一位: <code class="language-plaintext highlighter-rouge">[&lt;/s&gt;, token1, token2, ...]</code> ，这个偏移一位的目的是为了接下来的mask比较好做。各个 predict element 的生成是并行的<li>预测阶段（生成阶段）是需要多次调用 decoder 的部分的，在上一个 decoder 生成输出之后，如果不为结束符，则加入到 predicted_data 里 <code class="language-plaintext highlighter-rouge">[\&lt;/s&gt;, token1, token2, ...]</code>，继续扔进 decoder 里预测下一个 token。各个 predict element 的生成是串行的。</ul><li><p>加入 position encoding 信息，同 encoder 步骤 2。</p><li><p>这一步的 Multi-Head Attention 用的是 masked 的，就是 encoder 步骤 4 中将还没有 predict 的部分的 attention 给 mask 掉，这样就可以在那一步只获取之前的 weighted value，而丝毫不受未预测部分的 value 所影响；而至于在后面那个 Multi-Head Attention 为什么不用 mask？是因为那一步中的 key 和 value 是 encoder 的结果，包含的是整个序列的信息，与未预测的真实信息无关，所以不用 mask。其余的同 encoder 的 步骤 3-8。</p><li><p>接下来的一个 Multi-Head Attention 使用了 encoder 的结果，Query 是用上一个 layer 的结果求得的，Key 和 Value 是用 encoder 的结果求得的，<code class="language-plaintext highlighter-rouge">Residual + LayerNorm + Dropout</code>，得到 <code class="language-plaintext highlighter-rouge">multihead_outputs(seq_len, d_model)</code></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre>QueryMatrix(seq_len, d_k) = first_MultiLayer_output(seq_len, d_model) * Wq(d_model, d_k)
KeyMatrix(seq_len, d_k) = encoder_output(seq_len, d_model) * Wk(d_model, d_k)
ValueMatrix(seq_len, d_v) = encoder_output(seq_len, d_model) * Wv(d_model, d_v)
   
AttentionMatrix(seq_len, seq_len) = QueryMatrix(seq_len, d_k) * KeyMatrix.T(d_k, seq_len) / (d_k)^0.5
   
layer_outputs(seq_len, d_v) = AttentionMatrix(seq_len, seq_len) * ValueMatrix(seq_len, d_v)
</pre></table></code></div></div><li><p>FF layer 也是同 encoder 步骤9-10。</p><li><p>步骤 3-5 也是要重复 $N$ 次。</p><li><p>最后再 Linear tranformation 降为 token corpus 大小，softmax 求各个 token 的概率。</p></ol><hr /><h3 id="小细节"><span class="mr-2">小细节</span><a href="#小细节" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><h4 id="regularization"><span class="mr-2">Regularization</span><a href="#regularization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><ol><li><p><strong>Residual Dropout</strong>: 每一层前面都加了 dropout function，包括 encoder 和 decoder 求 <code class="language-plaintext highlighter-rouge">inputs_pos</code></p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-31-transformer-attention-is-all-you-need/截屏2022-03-31 17.29.27.png" alt="截屏2022-03-31 17.29.27" style="zoom:50%;" data-proofer-ignore> <em>Residual Dropout</em></p><li><p><strong>Label Smoothing</strong>: cross-entropy 中使标签平滑</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-31-transformer-attention-is-all-you-need/截屏2022-03-31 17.28.52.png" alt="截屏2022-03-31 17.28.52" style="zoom:50%;" data-proofer-ignore> <em>Label Smoothing</em></p></ol><h4 id="权重共享"><span class="mr-2">权重共享</span><a href="#权重共享" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><ol><li><p>Encoder 和 Decoder 间的 Embedding 层权重共享；</p><p>对于共有的一些 tokens 可以更好的表达；但是词表会很大</p><li><p>Decoder 中 Embedding 层和 FC 层权重共享</p><p>FC 层通过得到的向量，根据 embedding 层的权重反向求得每个 token 的概率</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># Decoder FC层定义，无 bias
</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">))</span>   <span class="c1"># nn.Linear 的权重部分定义
</span></pre></table></code></div></div></ol><h4 id="mask用到的地方"><span class="mr-2">Mask用到的地方</span><a href="#mask用到的地方" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><ol><li><p>因为所有的文本长度不一样，所以会有一些文本是经过padding处理的，padding 部分用 mask 遮盖</p><li><p>decoder 计算 attention 的时候，未预测部分由 mask 遮盖</p></ol><h4 id="multi-head-attention"><span class="mr-2">Multi-Head Attention</span><a href="#multi-head-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><ol><li><p>多头注意力机制是按照特征维度划分的，可以保证<strong>计算量不增加</strong>的情况下，提升特征捕捉能力</p><blockquote><p>Due to the reduced dimension of each head, the total computational cost is <strong>similar</strong> to that of single-head attention with full dimensionality.</p></blockquote><li><p>可以类比CNN中同时使用<strong>多个滤波器</strong>的作用，直观上讲，多头的注意力<strong>有助于网络捕捉到更丰富的特征/信息。</strong></p></ol><h4 id="decoder-训练和测试的输入输出差别"><span class="mr-2">decoder 训练和测试的输入输出差别</span><a href="#decoder-训练和测试的输入输出差别" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><ol><li><p>训练阶段中，decoder 的<strong>输入</strong>是真实输出序列<strong>向右偏移一位</strong>: <code class="language-plaintext highlighter-rouge">[&lt;\s&gt;, token1, token2, ...]</code>，在 decoder 的第一个 Multi-Head Attention 中生成 query-key attention 之后对未预测部分做了 mask，如以下 <code class="language-plaintext highlighter-rouge">AttentionMatrix</code> 表格，目的是为了预测下一个 token 时只用到<strong>当前及之前的 token 信息</strong>，例如预测 token1 时只用到了 <code class="language-plaintext highlighter-rouge">&lt;\s&gt;</code> 一个 token。那么为什么 decoder 中第二个 Multi-Head Attention 不用 mask 呢？是因为第二个 mask 的 key, value 都是 encoder 生成的，带着整个 sequence 的信息，用于生成下一个 token 是没问题的；在剩下的操作中 <code class="language-plaintext highlighter-rouge">(linear transformer &amp; normalization)</code> 都是<strong>按行处理</strong>，因此不会发生数据泄露。</p><div class="table-wrapper"><table><thead><tr><th>query\key<th>&lt;\s&gt;.key<th>token1.key<th>token2.key<th>token3.key<th>mask.key (padding)<tbody><tr><td>&lt;\s&gt;.query<td><strong>attention</strong><td>mask<td>mask<td>mask<td>mask<tr><td>token1.query<td><strong>attention</strong><td><strong>attention</strong><td>mask<td>mask<td>mask<tr><td>token2.query<td><strong>attention</strong><td><strong>attention</strong><td><strong>attention</strong><td>mask<td>mask<tr><td>token3.query<td><strong>attention</strong><td><strong>attention</strong><td><strong>attention</strong><td><strong>attention</strong><td>mask<tr><td>mask.query (padding)<td>mask<td>mask<td>mask<td>mask<td>mask</table></div><li><p>训练阶段中，decoder 的<strong>输出</strong>应该是 <code class="language-plaintext highlighter-rouge">(seq_len, k)</code>，每一行都是<strong>独立预测</strong>出来的，没有依赖前一个 token 的预测，例如预测 token5 是由真实 token1 到真实 token4 所生成的。loss function 就是 <code class="language-plaintext highlighter-rouge">predictMatrix (seq_len, vocab_size)</code> 和 <code class="language-plaintext highlighter-rouge">truthMatrix (seq_len, vocab_size)</code> 的 <code class="language-plaintext highlighter-rouge">cross-entropy</code>，其中 <code class="language-plaintext highlighter-rouge">truthMatrix</code> 使用的不是 onehot，是 <em>label smoothing</em>。</p><li><p>测试阶段中，decoder 的<strong>输入</strong>是 sequential 的，即每个 token 按先后顺序预测，根据已预测的 token 去预测新的 token，例如预测第一个 token 的输入应该是: <code class="language-plaintext highlighter-rouge">[&lt;\s&gt;, mask, ...]</code> 长度固定，<strong>输出</strong>应该就是: <code class="language-plaintext highlighter-rouge">[token1, ....]</code>。再预测下一个的时候直到预测到结束符就可以停下来了，这时就得到了完整的预测序列了。</p></ol><hr /><h2 id="论文笔记"><span class="mr-2">论文笔记</span><a href="#论文笔记" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="introduction"><span class="mr-2">Introduction:</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><blockquote><p>In this work we propose the Transformer, a model architecture <strong>eschewing recurrence</strong> and instead relying entirely on an attention mechanism to draw <strong>global dependencies</strong> between input and output. The Transformer allows for significantly more <strong>parallelization</strong> and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.</p></blockquote><hr /><h3 id="model-architecture"><span class="mr-2">Model Architecture:</span><a href="#model-architecture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><h4 id="encoder-and-decoder-stacks"><span class="mr-2">Encoder and Decoder Stacks</span><a href="#encoder-and-decoder-stacks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><p><strong>Encoder:</strong> Encoder 部分是由 $N=6$ 个相同的 layer 组成，每一个 layer 有两个部分，第一个部分是 multi-head self-attention，第二个部分是一个简单的全连接层。两个部分都用了残差连接 + layer normalization，即 $Output_{sub-layer} = LayerNorm(x+Sublayer(x))$ ，每一个输出的维度都相同 $d_{model} = 512$ 。</p><p><strong>Decoder:</strong> Decoder 部分也是由 $N=6$ 个相同的 layer 组成，与 encoder 不同的是每个 layer 中有一个额外的 Masked Multi-Head Attention Sub-layer，这里的 Mask 是用来遮盖待预测的序列的，只允许使用之前的序列进行预测；而第二个 Multi-Head Attention 是和 encoder 输出部分 concat 之后一起做 self-attention。</p><h4 id="attention"><span class="mr-2">Attention</span><a href="#attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><p>Attention 的功能是把一组 input 映射成对应的 query, key, value，利用 query 和各个输入的 key 求出对各个 input 的权重，再根据权重求出 weighted value 的集成，得到最后 query 对应 input 的 output：</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-31-transformer-attention-is-all-you-need/截屏2022-03-31 15.14.52.png" alt="截屏2022-03-31 15.14.52" style="zoom:50%;" data-proofer-ignore> <em>left: Scaled Dot-Product Attention, right: Multi-Head Attention</em></p><h5 id="scaled-dot-product-attention"><span class="mr-2">Scaled Dot-Product Attention</span><a href="#scaled-dot-product-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5></h5><p>上图左边是 Scaled Dot-Product Attention，输入是 k 维的 query, key 和 v 维的 value，计算 query 和所有 key 的点积再除以 $\sqrt{d_{k}} $ ，在用 softmax 取得各个 value 的权重。式子如下：</p>\[\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V\]<p>有两种常用的 attention: <strong>dot-product attention</strong> and <strong>additive attention</strong>，这里就是用的 dot-product attention + scaling factor。为了不让点积越来越大，通过除以维度的开方来降低 softmax 的差距。</p><h5 id="multi-head-attention-1"><span class="mr-2">Multi-Head Attention</span><a href="#multi-head-attention-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5></h5><p>通过使用多个 self-attentions 机制来获取子空间更多方面的表达，用 concat 把多个 self-attentions 的输出集合起来再用一个 linear transformer 转换到原来的维度，如下图。在论文中使用了 8 层 attention layer。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-31-transformer-attention-is-all-you-need/截屏2022-03-31 15.46.26.png" alt="截屏2022-03-31 15.46.26" style="zoom:50%;" data-proofer-ignore> <em>Multi-Head Attention</em></p><h5 id="applications-of-attention-in-model"><span class="mr-2">Applications of Attention in Model</span><a href="#applications-of-attention-in-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5></h5><ul><li>在 encoder-decoder attention 层，query 是来自 decoder 上一层的，value 和 key 是来自 encoder 的输出的，这使得解码器中的每个位置都能关注到输入序列中的所有位置。<li>encoder 的 self-attention 中，query, key, value 都是来自于上一层的输出，编码器中的每个位置都可以关注到编码器前一层的所有位置。<li>同样，decoder 中的自我关注层允许解码器中的每个位置关注已经预测的位置，防止观察到待预测值</ul><h4 id="position-wise-feed-forward-networks"><span class="mr-2">Position-wise Feed-Forward Networks</span><a href="#position-wise-feed-forward-networks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><p>fully connected feed-forward network 包含了两个 linear transformer 和 ReLU 激活函数在中间</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-31-transformer-attention-is-all-you-need/截屏2022-03-31 17.49.52.png" alt="截屏2022-03-31 17.49.52" style="zoom:50%;" data-proofer-ignore> <em>FFN</em></p><h4 id="embeddings-and-softmax"><span class="mr-2">Embeddings and Softmax</span><a href="#embeddings-and-softmax" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><p>与其他序列转换模型类似，使用学习到的 emebddings 来转换输入的 tokens 和输出tokens 转换为维度为 $d_{model}$ 的向量。还使用 linear transformation 和 softmax 函数将 decoder 的输出转换为预测的下一个 token 的概率。在模型中，在两个 embedding 层和 pre-softmax linear transformation 之间共享相同的权重矩阵。在 embedding 层，会将其乘以权重 $\sqrt {d_{model}}$ 。</p><h4 id="positional-encoding"><span class="mr-2">Positional Encoding</span><a href="#positional-encoding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><p>为了保持序列顺序信息，必须添加 position encoding 信息到 input embeddings 中，为了可以相加，position encoding 和 embedding 维度相同。在论文中，使用的是公式：</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-31-transformer-attention-is-all-you-need/截屏2022-03-31 18.00.25.png" alt="截屏2022-03-31 18.00.25" style="zoom:50%;" data-proofer-ignore> <em>Positional Encoding</em></p><hr /><h3 id="why-self-attention"><span class="mr-2">Why Self-Attention</span><a href="#why-self-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ol><li>每一层的计算复杂度<li>并行能力<li>解决序列长距离依赖问题</ol><hr /><h3 id="conclusion"><span class="mr-2">Conclusion</span><a href="#conclusion" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><blockquote><p>In this work, we presented the Transformer, <strong>the first sequence transduction model based entirely on attention</strong>, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.</p><p>For translation tasks, the Transformer can be trained <strong>significantly faster</strong> than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.</p><p>We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.</p></blockquote></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/papers/'>papers</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/transformer/" class="post-tag no-text-decoration" >Transformer</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Transformer: Attention Is All You Need - JuhaoLiang&amp;url=/posts/transformer-attention-is-all-you-need/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Transformer: Attention Is All You Need - JuhaoLiang&amp;u=/posts/transformer-attention-is-all-you-need/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=/posts/transformer-attention-is-all-you-need/&amp;text=Transformer: Attention Is All You Need - JuhaoLiang" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tools/">tools</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/model-ensemble/">Model Ensemble</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/transfer-learning/">Transfer Learning</a> <a class="post-tag" href="/tags/xgboost/">XGBoost</a> <a class="post-tag" href="/tags/gcn/">GCN</a> <a class="post-tag" href="/tags/gradient-boost/">Gradient Boost</a> <a class="post-tag" href="/tags/script/">script</a> <a class="post-tag" href="/tags/semi-supervised-learning/">Semi-supervised Learning</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/graph-convolutional-networks-for-text-classification/"><div class="card-body"> <em class="timeago small" data-ts="1647785700" > 2022-03-20 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Graph Convolutional Networks for Text Classification</h3><div class="text-muted small"><p> Paper 阅读笔记 Graph Convolutional Netowrks for Text Classification 构建 双层 GCN 异构图，words 和 documents 做节点，words 之间的 edge 用词共现信息表示，word 和 documents 之间的 edge 用 tf-idf 表示。然后做把文本分类转换为点分类，该方法可以在小比例的标记文档中实现强...</p></div></div></a></div><div class="card"> <a href="/posts/xgboost-a-scalable-tree-boosting-system/"><div class="card-body"> <em class="timeago small" data-ts="1648341960" > 2022-03-27 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>XGBoost: A Scalable Tree Boosting System</h3><div class="text-muted small"><p> Paper 阅读笔记 XGBoost: A Scalable Tree Boosting System 之前写 ensemble 那篇笔记的时候碰上了这些 boosting 的方法，看了几个讲解 XGBoost 的视频后还是不太了解，于是开始阅读这篇 paper，读了差不多七八个小时才读完…. 看完后感觉基本的概念都了解一点，需要阅读或者实际使用一下这个系统才能更好的体会到各个技术点的作...</p></div></div></a></div><div class="card"> <a href="/posts/%E5%85%B3%E4%BA%8E%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9A%8F%E7%AC%94/"><div class="card-body"> <em class="timeago small" data-ts="1652704680" > 2022-05-16 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>关于自监督学习模型的随笔</h3><div class="text-muted small"><p> 关于自监督学习模型的随笔 吃完饭休息一会，看李宏毅老师视频做的随笔 参考资料：惡搞自督導式學習模型 BERT 的三個故事, Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model Overview 视频讲述...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/xgboost-a-scalable-tree-boosting-system/" class="btn btn-outline-primary" prompt="Older"><p>XGBoost: A Scalable Tree Boosting System</p></a> <a href="/posts/bert-model/" class="btn btn-outline-primary" prompt="Newer"><p>BERT Model</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/JuhaoLiang1997">JuhaoLiang</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tools/">tools</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/model-ensemble/">Model Ensemble</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/transfer-learning/">Transfer Learning</a> <a class="post-tag" href="/tags/xgboost/">XGBoost</a> <a class="post-tag" href="/tags/gcn/">GCN</a> <a class="post-tag" href="/tags/gradient-boost/">Gradient Boost</a> <a class="post-tag" href="/tags/script/">script</a> <a class="post-tag" href="/tags/semi-supervised-learning/">Semi-supervised Learning</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-77HPQKSLY9"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-77HPQKSLY9'); }); </script>
