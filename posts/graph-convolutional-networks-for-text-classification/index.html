<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Graph Convolutional Networks for Text Classification" /><meta property="og:locale" content="en" /><meta name="description" content="Paper 阅读笔记" /><meta property="og:description" content="Paper 阅读笔记" /><link rel="canonical" href="/posts/graph-convolutional-networks-for-text-classification/" /><meta property="og:url" content="/posts/graph-convolutional-networks-for-text-classification/" /><meta property="og:site_name" content="JuhaoLiang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-03-20T22:15:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Graph Convolutional Networks for Text Classification" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-03-20T22:15:00+08:00","datePublished":"2022-03-20T22:15:00+08:00","description":"Paper 阅读笔记","headline":"Graph Convolutional Networks for Text Classification","mainEntityOfPage":{"@type":"WebPage","@id":"/posts/graph-convolutional-networks-for-text-classification/"},"url":"/posts/graph-convolutional-networks-for-text-classification/"}</script><title>Graph Convolutional Networks for Text Classification | JuhaoLiang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JuhaoLiang"><meta name="application-name" content="JuhaoLiang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images/avatar.jpg " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JuhaoLiang</a></div><div class="site-subtitle font-italic">记录日常技术博客</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/JuhaoLiang1997" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['JuhaoLiang1997','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Graph Convolutional Networks for Text Classification</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Graph Convolutional Networks for Text Classification</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/JuhaoLiang1997">JuhaoLiang</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1647785700" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-03-20 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2275 words"> <em>12 min</em> read</span></div></div></div><div class="post-content"><h1 id="paper-阅读笔记">Paper 阅读笔记</h1><p><a href="https://arxiv.org/abs/1809.05679">Graph Convolutional Netowrks for Text Classification</a></p><p>构建 双层 GCN 异构图，words 和 documents 做节点，words 之间的 edge 用词共现信息表示，word 和 documents 之间的 edge 用 tf-idf 表示。然后做把文本分类转换为点分类，该方法可以在小比例的标记文档中实现强大的分类性能，可以同时学习单词和文档节点 embedding。<a href="https://github.com/yao8839836/text_gcn">源码</a> 在这里。</p><p><strong>Keywords</strong>: NLP, Text GCN, GCN, Text Classification, Graph</p><hr /><h2 id="contributions"><span class="mr-2">Contributions：</span><a href="#contributions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>提出了一个用于文本分类新的图神经网络方法，可用于联合学习 <strong>word 和 document embeddings</strong><li>几个基准数据集显示这个方法 state-of-the-art，没有用到预训练词向量和额外知识。</ul><hr /><h2 id="related-work"><span class="mr-2">Related Work：</span><a href="#related-work" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><p>Deep learning text classification studies can be categorized into two groups.</p></blockquote><ul><li>Models based on word embeddings<li>Model employed deep neural networks</ul><hr /><h2 id="method"><span class="mr-2">Method：</span><a href="#method" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="gcn"><span class="mr-2">GCN</span><a href="#gcn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li><p>graph $ G = (V, E)$ where $V, E$ are sets of nodes and edges</p><li><p>$ (v, v) \in E $ for any $v$, every node is assumed to be connected to itself</p><li><p>$X \in \mathbb{R}^{n \times m}$ 是特征矩阵，n 个节点 m 维特征；$X_{v} \in \mathbb{R}^{m}$ 表示 $v$ 的特征向量</p><li><p>$A$ 是 $G$ 的邻接矩阵，度矩阵 $D$ where $D_{i i}=\sum_{j} A_{i j}$ 就是邻接矩阵求每个点的度 (邻接数)；$A$ 的主对角线都设为 1 因为每个点都连接自己</p><li><p>单层 GCN 时每个点都只能获取距离为一的邻接点的信息，而多层 GCN 叠加的时候就可以把更广的范围信息整合起来</p><li><p>单层 GCN，新的 $k$-dimensional node 的特征矩阵 $L^{(1)} \in \mathbb{R}^{n \times k}$ 可以被计算为</p><p>$L^{(1)}=\rho\left(\tilde{A} X W_{0}\right)$ (1) :+1::+1:</p><p>where $\tilde{A}=D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$ is normalized symmetric adjacency matrix and $W_{0} \in \mathbb{R}^{m \times k}$ is a weight matrix, $\rho$ 是激活函数</p><li><p>多层 GCN：</p><p>$L^{(j+1)}=\rho\left(\tilde{A} L^{(j)} W_{j}\right)$ (2)</p><p>where $j$ denotes the layer number and $L^{(0)} = X$</p></ul><h3 id="text-gcn"><span class="mr-2">Text GCN</span><a href="#text-gcn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li><p>建立异构图可以把<strong>词共现信息</strong>显性的表现在graph中，方便图卷积</p><li><p>图的点数量 V 等于 documents 数 + words 数 (corpus size + vocabulary size)</p><li><p>输入时简单设置特征矩阵 $X = I$ ，每个word/document都表示为 one-hot vector</p><li><p>Document node 和 word node 的边权重为 word 在 document 中的 TF-IDF，他们测试过 TF-IDF weight 比只用 term frequency 的要好</p><li><p>使用 point-wise mutual information (PMI) 计算两个 word node edge 的权重，PMI 要比<strong>共现次数</strong>效果好</p><li><p>综上得到邻接矩阵 $A$</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-20-graph-convolutional-networks-for-text-classification/equation1.png" alt="equation1" style="zoom:50%;" data-proofer-ignore></p><li><p>PMI 值计算 word node $i, j$</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-20-graph-convolutional-networks-for-text-classification/equation2.png" alt="equation2" style="zoom:50%;" data-proofer-ignore></p><p>where #W(i) 是 sliding windows 的包含 word $i$ 的数量</p><p>#W(i, j) 是包含 word $i, j$ 的 sliding windows 数量</p><p>#W 是 sliding windows 在 corpus 的总数量</p><li><p>一个正的 PMI 值说明语义上两个词强关联，反之弱关联；因此只把 PMI 为正的边添加到 graph 中（threshold -&gt; sparse）</p><li><p>建立好 graph 之后把它放进两层 GCN 中：第一层根据公式 (1) 算出新的特征向量，再扔进第二层公式(1) 又算出新的特征向量，然后扔进 <em>softmax</em> 里生成等同标签数的概率，进行分类</p><p>$Z=\operatorname{softmax}\left(\tilde{A} \operatorname{ReLU}\left(\tilde{A} X W_{0}\right) W_{1}\right)$ （7）</p><p>loss function 就用的 cross-entropy，只计算了 labeled document 节点:</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-20-graph-convolutional-networks-for-text-classification/equation3.png" alt="equation3" style="zoom:50%;" data-proofer-ignore>(8)</p><p>where $\mathcal{Y}_{D}$ is the set of labeled document indices</p><p>$F$ is the dimension of the output features</p><p>$Y$ is the label indicator matrix (label 矩阵)</p><li><p>公式 (7) 的 $W_{0}, W_{1}$ 可以用 gradient descent 训练，示意图如下</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-20-graph-convolutional-networks-for-text-classification/Schematic of Text GCN.png" alt="Schematic of Text GCN" style="zoom:50%;" data-proofer-ignore> <em>Schematic of Text GCN</em></p><li><p>两层 GCN 最多允许 距离为 2 的信息传递，即使没有 document 之间的连边，两层 GCn 也允许 documents 之间传递信息。他们实验说两层的要比一层的要好，更多层也没有提升效果。</p></ul><hr /><h2 id="experiments"><span class="mr-2">Experiments：</span><a href="#experiments" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="determine"><span class="mr-2">Determine:</span><a href="#determine" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>Can our model achieve satisfactory results in text classification, even with limited labeled data? 能不能使用少量的 labeled data 训练得到不错的结果，在文本分类中<li>Can our model learn predictive word and document embeddings?</ul><h3 id="baselines"><span class="mr-2">Baselines</span><a href="#baselines" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li><strong>TF-IDF + LR</strong>：tf-idf 特征 + Logistic Regression classifier<li><strong>CNN</strong><ul><li>CNN-rand：随机初始化embedding<li>CNN-non-static：无监督学习的 embedding，训练中 fine tuned</ul><li><strong>LSTM</strong>：有/无 预训练词向量<li><strong>Bi-LSTM</strong>：预训练词向量<li><strong>PV-DBOW</strong>：BOW，Logistic Regression Classifier<li><strong>PTE</strong>：词向量 average 做 doc向量<li><strong>fastText</strong>：word/n-grams 向量均值做 doc向量 + linear classifier<li><strong>SWEM</strong>：词向量 做 simple pooling strategies<li><strong>LEAM</strong>：词和标签在同一空间进行分类，用到了 label description<li><strong>Graph-CNN-C</strong>：graph CNN + shev filter<li><strong>Graph-CNN-S</strong>：graph CNN + Spline filter<li><strong>Graph-CNN-F</strong>：graph CNN + Fourier filter</ul><h3 id="datasets"><span class="mr-2">Datasets</span><a href="#datasets" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>用了五个数据集：20-Newsgroups (20NG), Ohsumed, R52 and R8 of Reuters 21578 and Movie Review (MR).</p><h3 id="preprocess"><span class="mr-2">Preprocess</span><a href="#preprocess" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li><p>Cleaning &amp; Tokenizing</p><li><p>Removed stop words NLTK, low frequency words less than 5 times for some datasets. MR 数据集太小了就没移除低频词</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-20-graph-convolutional-networks-for-text-classification/Summary statistics of datasets.png" alt="Summary statistics of datasets" style="zoom:50%;" data-proofer-ignore> <em>Summary statistics of datasets</em></p></ul><h3 id="settings"><span class="mr-2">Settings</span><a href="#settings" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>Text GCN, <strong>embedding size</strong> of first convolution as 200, <strong>window size</strong> as 20, <strong>learning rate</strong> 0.02, <strong>dropout rate</strong> 0.5, <strong>L2 loss</strong> 0. 10 fold Cross-validation, 200 <strong>epochs</strong>, <strong>Adam optimizer</strong>, <strong>early stopping</strong> (10 epoch)<li>Baseline model with default parameter setting as original papers<li>Baseline pretrain word embeddings: 300-dimensional GloVe</ul><h3 id="performance"><span class="mr-2">Performance</span><a href="#performance" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-20-graph-convolutional-networks-for-text-classification/ Test Accuracy.png" alt=" Test Accuracy" style="zoom:50%;" data-proofer-ignore> <em>Test Accuracy</em></p><ul><li>Text GCN performs the best and significantly outperforms all baseline models on four datasets except MR. 说明长文本数据集表现不错</ul><blockquote><p>Word nodes can gather comprehensive document label information and act as bridges or key paths in the graph, so that label information can be propagated to the entire graph.</p></blockquote><ul><li>这里作者说的很棒，document 的标签信息影响到了词向量的表达</ul><blockquote><p>However, we also observed that Text GCN did not outperform CNN and LSTM-based models on MR. This is because GCN ignores word orders that are very useful in sentiment classification, while CNN and LSTM model consecutive word sequences explicitly.</p></blockquote><ul><li>总结了在 MR 数据集上表现不如 CNN 和 LSTM 的原因，没有利用到语序，结果在情感分类表现就没有序列模型那么好了。</ul><blockquote><p>Another reason is that the edges in MR text graph are fewer than other text graphs, which limits the message passing among the nodes. There are only few document-word edges because the documents are very short. The number of word-word edges is also limited due to the small number of sliding windows.</p></blockquote><ul><li>再有就是在短文本 corpus 里，document 连接的 word node 就很少，导致标签信息很难传开，这也和 sliding windows 大小有关。</ul><h4 id="parameter-sensitivity"><span class="mr-2">Parameter Sensitivity</span><a href="#parameter-sensitivity" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><ul><li>作者测试了 不同 sliding windows 大小对模型的影响，实验表明过大和过小都会有负影响。说明了太小的不能生成足够的词共现信息，太大的导致不关联词语也连接上边，减少了差异性<li>也测试了第一层 GCN 不同维度的 embeddings，太小的维度导致 标签信息无法传开；太大的维度导致训练成本增加，同时性能也没提升。</ul><h4 id="effects-of-the-size-of-labels-data"><span class="mr-2">Effects of the Size of Labels Data</span><a href="#effects-of-the-size-of-labels-data" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><ul><li>在 20NG 和 R8 数据集上做了几个模型在不同比例 labeled data 的数据集情况下的 accuracy 分析。结果说明了 GCN 在低标签数据集中也可以有很好的表现 20% training documents</ul><h4 id="document-embeddings"><span class="mr-2">Document Embeddings</span><a href="#document-embeddings" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><ul><li><p>t-SNE 分析了生成的 document embeddings，和其他几个模型做了对比。说是 Text GCN 可以学习到更有区分度的 embeddings</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-20-graph-convolutional-networks-for-text-classification/document embeddings.png" alt="document embeddings" style="zoom:50%;" data-proofer-ignore> <em>document embeddings</em></p></ul><h4 id="word-embeddings"><span class="mr-2">Word Embeddings</span><a href="#word-embeddings" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><ul><li>同样用 t-SNE 对比 Text GCN 所生成的 word embeddings 和其他模型。</ul><h3 id="discussion"><span class="mr-2">Discussion</span><a href="#discussion" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><blockquote><p>From experimental results, we can see the proposed Text GCN can achieve strong text classification re- sults and learn predictive document and word embeddings. However, a major limitation of this study is that the GCN model is inherently transductive, in which test document nodes (without labels) are included in GCN training. Thus Text GCN could not quickly generate embeddings and make prediction for unseen test documents. Possible solutions to the problem are introducing inductive (Hamilton, Ying, and Leskovec 2017) or fast GCN model (Chen, Ma, and Xiao 2018).</p></blockquote><ul><li>作者所提出的 Text GCN 可以达到很好的文本分类结果<li>主要的问题是 GCN 本质上是传导的模型，测试数据是被包括在训练中的，因此 Text GCN 不能很快的生成 embeddings 以及作出预测<li>可能的解决方案是 引入 inductive or fast GCn model</ul><hr /><h2 id="conclusion-nad-future-work"><span class="mr-2">Conclusion nad Future Work</span><a href="#conclusion-nad-future-work" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><p>In this study, we propose a novel text classification method termed Text Graph Convolutional Networks (Text GCN). We build a heterogeneous word document graph for a whole corpus and turn document classification into a node classification problem. Text GCN can capture global word co-occurrence information and utilize limited labeled documents well. A simple two-layer Text GCN demonstrates promising results by outperforming numerous state-of-the- art methods on multiple benchmark datasets.</p><p>In addition to <strong>generalizing Text GCN model to inductive settings</strong>, some interesting future directions include improving the classification performance using <strong>attention mechanisms</strong> (Velicˇkovic ́ et al. 2018) and developing <strong>unsupervised text GCN framework</strong> for representation learning on large scale unlabeled text data.</p></blockquote><hr /><h2 id="个人总结"><span class="mr-2">个人总结：</span><a href="#个人总结" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li><p>本文的方法只用了词词连边、词文连边，词和词之间都是有连边的，相当于把 document 放进了 word 的 graph 里，也就是用 word embedding 表达 doc embedding，本质上是 Bag-of-words model；多了 document 之间信息的传递，把 word2vec 和 doc2vec 两个步骤合在了一起，使得 word 和 document 的 embedding 相互影响。也正如 contribution 所说，可以不利用预训练的词向量和预先知识，同时计算 word 和 document 的 embedding。</p><li><p>Text GCN 出现新的 document 时会影响到 word 和 word 之间权重，所以需要重新训练调整边的权重，不能很快的得到新 document 的 embedding 的分类结果。</p></ul><hr /><h2 id="闲谈"><span class="mr-2">闲谈：</span><a href="#闲谈" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li><p>曾几何时我本科毕业论文题目也是 异构图神经网络判别推特水军 … 不过我没用到文本信息，只是从社交网络 graph 来判别…</p><li><p>读这篇 paper 的初衷是完成 Text as Data 这门课的 coursework… 有空把剩下几篇也做一下笔记</p><ul><li><p>Class-Based n-gram Models of Natural Language</p><li><p>Enriching Word Vectors with Subword Information</p><li><p>GloVe-Global Vectors for Word Representation</p><li><p>Graph Convolutional Networks for Text Classification</p><li><p>Latent Dirichlet Allocation</p><li><p>Translating Embeddings for Modeling Multi-relational Data</p><li><p>Word representations- A simple and general method for semi-supervised learning</p></ul></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/papers/'>papers</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >NLP</a> <a href="/tags/gcn/" class="post-tag no-text-decoration" >GCN</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Graph Convolutional Networks for Text Classification - JuhaoLiang&amp;url=/posts/graph-convolutional-networks-for-text-classification/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Graph Convolutional Networks for Text Classification - JuhaoLiang&amp;u=/posts/graph-convolutional-networks-for-text-classification/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=/posts/graph-convolutional-networks-for-text-classification/&amp;text=Graph Convolutional Networks for Text Classification - JuhaoLiang" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tools/">tools</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/model-ensemble/">Model Ensemble</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/transfer-learning/">Transfer Learning</a> <a class="post-tag" href="/tags/xgboost/">XGBoost</a> <a class="post-tag" href="/tags/gcn/">GCN</a> <a class="post-tag" href="/tags/gradient-boost/">Gradient Boost</a> <a class="post-tag" href="/tags/script/">script</a> <a class="post-tag" href="/tags/semi-supervised-learning/">Semi-supervised Learning</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/%E5%85%B3%E4%BA%8E%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9A%8F%E7%AC%94/"><div class="card-body"> <em class="timeago small" data-ts="1652704680" > 2022-05-16 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>关于自监督学习模型的随笔</h3><div class="text-muted small"><p> 关于自监督学习模型的随笔 吃完饭休息一会，看李宏毅老师视频做的随笔 参考资料：惡搞自督導式學習模型 BERT 的三個故事, Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model Overview 视频讲述...</p></div></div></a></div><div class="card"> <a href="/posts/xgboost-a-scalable-tree-boosting-system/"><div class="card-body"> <em class="timeago small" data-ts="1648341960" > 2022-03-27 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>XGBoost: A Scalable Tree Boosting System</h3><div class="text-muted small"><p> Paper 阅读笔记 XGBoost: A Scalable Tree Boosting System 之前写 ensemble 那篇笔记的时候碰上了这些 boosting 的方法，看了几个讲解 XGBoost 的视频后还是不太了解，于是开始阅读这篇 paper，读了差不多七八个小时才读完…. 看完后感觉基本的概念都了解一点，需要阅读或者实际使用一下这个系统才能更好的体会到各个技术点的作...</p></div></div></a></div><div class="card"> <a href="/posts/transformer-attention-is-all-you-need/"><div class="card-body"> <em class="timeago small" data-ts="1648734120" > 2022-03-31 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Transformer: Attention Is All You Need</h3><div class="text-muted small"><p> Paper 阅读笔记 Attention Is All You Need, 【機器學習2021】Transformer, transformer implementation, 代码笔记, Transformer 面经总结 transformer 主要还是利用了 self-attention 的机制，打破了传统 rnn 基于序列的线性训练方法，增强其并行运算能力，克服长距离依赖问题；但...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/gitpage-blog-setup/" class="btn btn-outline-primary" prompt="Older"><p>Gitpage Blog Setup</p></a> <a href="/posts/Tools/" class="btn btn-outline-primary" prompt="Newer"><p>Tools</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/JuhaoLiang1997">JuhaoLiang</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tools/">tools</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/model-ensemble/">Model Ensemble</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/transfer-learning/">Transfer Learning</a> <a class="post-tag" href="/tags/xgboost/">XGBoost</a> <a class="post-tag" href="/tags/gcn/">GCN</a> <a class="post-tag" href="/tags/gradient-boost/">Gradient Boost</a> <a class="post-tag" href="/tags/script/">script</a> <a class="post-tag" href="/tags/semi-supervised-learning/">Semi-supervised Learning</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-77HPQKSLY9"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-77HPQKSLY9'); }); </script>
