<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="XGBoost: A Scalable Tree Boosting System" /><meta property="og:locale" content="en" /><meta name="description" content="Paper 阅读笔记" /><meta property="og:description" content="Paper 阅读笔记" /><link rel="canonical" href="/posts/xgboost-a-scalable-tree-boosting-system/" /><meta property="og:url" content="/posts/xgboost-a-scalable-tree-boosting-system/" /><meta property="og:site_name" content="JuhaoLiang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-03-27T08:46:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="XGBoost: A Scalable Tree Boosting System" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-03-27T08:46:00+08:00","datePublished":"2022-03-27T08:46:00+08:00","description":"Paper 阅读笔记","headline":"XGBoost: A Scalable Tree Boosting System","mainEntityOfPage":{"@type":"WebPage","@id":"/posts/xgboost-a-scalable-tree-boosting-system/"},"url":"/posts/xgboost-a-scalable-tree-boosting-system/"}</script><title>XGBoost: A Scalable Tree Boosting System | JuhaoLiang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JuhaoLiang"><meta name="application-name" content="JuhaoLiang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images/avatar.jpg " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JuhaoLiang</a></div><div class="site-subtitle font-italic">记录日常技术博客</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/JuhaoLiang1997" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['JuhaoLiang1997','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>XGBoost: A Scalable Tree Boosting System</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>XGBoost: A Scalable Tree Boosting System</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/JuhaoLiang1997">JuhaoLiang</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1648341960" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-03-27 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3897 words"> <em>21 min</em> read</span></div></div></div><div class="post-content"><h1 id="paper-阅读笔记">Paper 阅读笔记</h1><p><a href="https://arxiv.org/abs/1603.02754">XGBoost: A Scalable Tree Boosting System</a></p><p>之前写 ensemble 那篇笔记的时候碰上了这些 boosting 的方法，看了几个讲解 XGBoost 的视频后还是不太了解，于是开始阅读这篇 paper，读了差不多七八个小时才读完…. 看完后感觉基本的概念都了解一点，需要阅读或者实际使用一下这个系统才能更好的体会到各个技术点的作用。</p><hr /><h2 id="contributions"><span class="mr-2">Contributions:</span><a href="#contributions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><ul><li>We design and build a <strong>highly scalable end-to-end tree boosting system</strong>.<li>We propose a theoretically justified <strong>weighted quantile sketch</strong> for efficient proposal calculation.<li>We introduce a novel <strong>sparsity-aware algorithm</strong> for <strong>parallel tree learning</strong>.<li>We propose an effective <strong>cache-aware block structure</strong> for <strong>out-of-core tree learning</strong>.</ul></blockquote><hr /><h2 id="tree-boosting"><span class="mr-2">Tree Boosting</span><a href="#tree-boosting" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="正则化的目标函数-regularized-learning-objective"><span class="mr-2">正则化的目标函数 Regularized Learning Objective</span><a href="#正则化的目标函数-regularized-learning-objective" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 02.34.36.png" alt="截屏2022-03-27 02.34.36" style="zoom:50%;" data-proofer-ignore> <em>预测值是所有树的预测之和</em></p><p>在给定的数据集 $\mathcal{D}$ 中，有 $n$ 条数据 $m$ 个特征，Tree Boosting 中有 $K$ 棵<strong>回归树 (CART树)</strong>，每个回归树的叶子结点都有数值。对于每个输入 $x$，可以使用 $q$ （树结构）计算出所对应的叶子结点，叶子结点的值 $w$ 即为预测结果 $\hat y$。而在整个 ensemble 模型中，所有树的预测结果相加就是最终结果 $\hat y$，如式 (1) 所示。这里可以理解为每一棵树都是为了弥补前一棵树的不足（残差）而训练：$y_{i} = y - y_{i-1}$，这也意味着在 boosting 没有办法像 bagging 的 ensemble 方法一样<strong>并行的训练所有树</strong>。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 02.04.06.png" alt="截屏2022-03-27 02.04.06" style="zoom:50%;" data-proofer-ignore> <em>预测值</em></p><p>得到预测值之后，怎样去训练模型使得预测值和真实值更接近呢？通过 minimize 以下<strong>目标函数</strong>训练模型，目标函数由两部分组成：</p><ul><li>第一部分 $l$ 是损失函数 <strong>loss function</strong>，用于衡量预测值和实际值的差距，这个是可以自定的，只要可以<strong>一阶二阶可求导</strong>的凸函数就好。<li>第二项 $\Omega$ 是惩罚项/正则项 <strong>penalty</strong>，用于限制模型复杂程度，防止过拟合。其中 $T$ 是叶子结点个数，$\omega$ 是叶子结点值的和， $\gamma$ 和 $\lambda$ 是系数，可以自己调整权重。</ul><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 02.19.58.png" alt="截屏2022-03-27 02.19.58" style="zoom:50%;" data-proofer-ignore> <em>目标函数</em></p><h3 id="gradient-tree-boosting"><span class="mr-2">Gradient Tree Boosting</span><a href="#gradient-tree-boosting" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>因为式 (2) 中有函数作为参数，所以很难使用传统的方法去优化这个函数。以下式子是在第 $t$ 棵树中的目标函数，其中 $ \hat {y} _ {i} ^ {(t-1)}$ 是第 $i$ 条数据在第 $t-1$ 棵树中的预测值。这里就是式 (2) 中 $\hat y_{i} = \hat y_{i}^{t-1}+f_{t}(x_{i})$ ，也就是补残差。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 02.34.13.png" alt="截屏2022-03-27 02.34.13" style="zoom:50%;" data-proofer-ignore> <em>目标函数</em></p><p>然后使用<strong><a href="https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F">二阶泰勒展开</a></strong>/<strong><a href="https://zh.wikipedia.org/wiki/%E7%89%9B%E9%A1%BF%E6%B3%95">牛顿法</a></strong>去近似表达这个目标函数，方便后续更高效的优化目标函数，得到新的目标函数</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 02.43.17.png" alt="截屏2022-03-27 02.43.17" style="zoom:50%;" data-proofer-ignore> <em>泰勒展开式</em></p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 02.44.16.png" alt="截屏2022-03-27 02.44.16" style="zoom:50%;" data-proofer-ignore> <em>近似的目标函数</em></p><p>其中就是把 $\hat y_{i}^{t-1}$ 视为泰勒式中的 $a$，把 $f_{t}(x_{i})$ 视为 $(x-a)$ ，$g_{i}$ 是目标函数对上一棵树的预测值 $\hat y_{i}^{t-1}$ 的一阶导数 ${f}’(a)$，$h_{i}$ 为二阶导数 ${f}’’ (a)$。移除<strong>常数项</strong> $l(y _ {i} , \hat y ^ {t-1})$ 之后获得以下第 $t$ 棵树的目标函数式 (3)</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 02.52.27.png" alt="截屏2022-03-27 02.52.27" style="zoom:50%;" data-proofer-ignore> <em>目标函数</em></p><p>接着，定义 $I_{j} = {i \mid q\left(\mathbf{x}_{i}\right) = j}$ 为叶子结点 $j$ 的数据集合，将正则项 $\Omega$ 代入到目标函数中</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 03.04.19.png" alt="截屏2022-03-27 03.04.19" style="zoom:20%;" data-proofer-ignore> <em>正则项</em></p><p>将<strong>用数据求和</strong>改成<strong>用叶子结点的数据集合求和</strong>，即 \(\sum_{i=1}^{n}g_{i} f_{t}\left(\mathbf{x}_{i}\right)=\sum_{j=1}^{T}\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}\) ，合并同类项得到式 (4)</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 02.55.43.png" alt="截屏2022-03-27 02.55.43" style="zoom:50%;" data-proofer-ignore> <em>目标函数</em></p><p>接着，在式 (4) 中把 $w_{j}$ 视为变量，目标函数即为<strong>一元二次方程</strong>，取 $-\frac{b}{2a} $ 时为最优解，所以求得叶子结点 $j$ 的最优值 $w_{j}^{*}$</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 03.07.15.png" alt="截屏2022-03-27 03.07.15" style="zoom:50%;" data-proofer-ignore> <em>最优叶子结点值</em></p><p>将最优解代入式 (4) 中，得到最优目标函数值；这个式子可以用来衡量树结构 $q$ 的优劣，利用这个式子就可以像决策树的信息增益一样进行<strong>特征选择</strong>，进而优化树的结构。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 03.13.08.png" alt="截屏2022-03-27 03.13.08" style="zoom:50%;" data-proofer-ignore> <em>目标函数</em></p><p>如何判断分割的效果呢？每次树进行分割时，假定左边数据集为 $I_{L}$ 和右边数据集为 $I_{R}$ ，当前结点的总数据集 $I=I_{L}+I_{R}$ ，使用以下式子去计算切分后<strong>目标函数减小的数值</strong>：左子树分数与右子树分数的和减去不分割情况下的分数以及加入新叶子节点引入的复杂度代价。 $\gamma$ 是抑制节点个数的，是节点分裂的阈值；$\lambda$ 是抑制节点值不要太大。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 03.21.40.png" alt="截屏2022-03-27 03.21.40" style="zoom:50%;" data-proofer-ignore> <em>分割后目标函数减小值</em></p><h3 id="shrinkage-and-column-subsampling"><span class="mr-2">Shrinkage and Column Subsampling</span><a href="#shrinkage-and-column-subsampling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>除了添加正则项外，还增加了两个额外的技术去防止过拟合。</p><ul><li>第一个是 <strong>shrinkage</strong>，shrinkage 会给每一个新增的叶子结点数值乘以 $\eta$ ，相当于优化器的<strong>学习率</strong>，它降低了每棵树的叶子结点对将来的树的影响。降低每棵树对模型的优化程度，利用更多的树慢慢的逼近结果，使得学习更加平缓，可以更好的避免过拟合。<li>第二个是 <strong>column (feature) subsampling</strong>，根据用户反馈，使用 column subsampling 可以比传统的 row subsampling 更加有效的防止过拟合，同时也加快了并行计算的速度。和<strong>随机森林</strong>的应用是一样的，支持列抽样可以降低过拟合，同时减少了计算量。</ul><hr /><h2 id="split-finding-algorithms"><span class="mr-2">Split Finding Algorithms</span><a href="#split-finding-algorithms" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="basic-exact-greedy-algorithm"><span class="mr-2">Basic Exact Greedy Algorithm</span><a href="#basic-exact-greedy-algorithm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>以上介绍了 boosting 方法，但是如何根据式 (7) 找到最好的分裂树的方法还是最大的问题。如果是在分裂时将所有的特征都计算一遍 $L_{\text {split }}$ 则称为 <em>exact greedy algorithm</em> 。如 Algorithm 1 所示，它遍历了所有可能的特征，根据式 (7) 找出最大的 $score$ ，作为其最优分裂方案，为了提升效率，它必须先对叶子结点按照输入值排序，再去累加 $G_{L}$ 和 $H_{L}$，这样效率太低了。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 14.43.06.png" alt="截屏2022-03-27 14.43.06" style="zoom:50%;" data-proofer-ignore> <em>Exact Greedy Algorithm</em></p><h3 id="approximate-algorithm"><span class="mr-2">Approximate Algorithm</span><a href="#approximate-algorithm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>Exact Greedy Algorithm 很强但当数据不能全部存入内存时<strong>很难高效</strong>地执行，而且在<strong>分布式计算</strong>中也会有问题。因此，Approximate Algorithm 应运而生。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 14.53.59.png" alt="截屏2022-03-27 14.53.59" style="zoom:50%;" data-proofer-ignore> <em>Approximate Algorithm</em></p><p>第一步根据特征分布选出<strong>候选分裂点</strong> (candidate splitting points)，然后将特征根据候选分裂点进行 split，再累加各个区域的特征的 $g_{j}$ 和 $h_{j}$，根据式 (7) 算出分裂效果，然后从这些候选分裂点中找出最佳方案。而候选分裂点密度越高时就越接近 Exact Greedy Algorithm，准确率也就越高；相反，密度越低，计算的越快，拟合程度低，防止过拟合就越好。</p><p>选取候选分裂点有两种方案。<strong>global variant</strong> 在生成树结构之前就提出了所有的 candidate splits，后续不更新。<strong>local variant</strong> 会在每次分裂结束后重新提出新的候选方案。Global 的方法可以减少提出候选方案的次数，然而每次的候选数量会比 local 的多。而 local 则是每次分裂结束后都重新选候选方案，对于更深的树，可以选出更加合适的 candidates。以下实验也说明了，在 global 给出足够多的 candidates 时，准确率也可以和 local 的一样。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 15.08.05.png" alt="截屏2022-03-27 15.08.05" style="zoom:50%;" data-proofer-ignore> <em>两种方案对比</em></p><h3 id="weighted-quantile-sketch"><span class="mr-2">Weighted Quantile Sketch</span><a href="#weighted-quantile-sketch" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>那么如果挑选候选分裂点呢？通常根据 feature 的值均匀的分裂。首先 $D_{k}={(x_{1 k}, h_{1}),(x_{2 k}, h_{2}) \cdots(x_{n k}, h_{n})}$ 表示第 $k$ 个 feature 的所有数据的值和二阶导数，然后定义 <em>rank functions</em> 式 (8)，表示第 $k$ 个特征中值小于 $z$ 的数据的比例，由 $h$ 加权。目的就是找到所有的 $z$ 使得每一个区间的比例约等于 $1/\varepsilon $ ，这个 $\varepsilon $ 是一个分裂密度的参数，值越大，表示区间越小，越接近 exact greedy algorithm。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 15.17.24.png" alt="截屏2022-03-27 15.17.24" style="zoom:50%;" data-proofer-ignore> <em>候选分裂点选取</em></p><p>为什么要用 $h$ 加权呢？这里将式 (3) 转化为以下式子，这里就是<strong>一元二次方程</strong>转为完全平方式。转换后的目标函数正是对 $g_{i}/h_{i}$ 的加权方差 (weighted squared loss)，权为 $h_{i}$ 。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 15.25.52.png" alt="截屏2022-03-27 15.25.52" style="zoom:50%;" data-proofer-ignore> <em>目标函数</em></p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 15.25.38.png" alt="截屏2022-03-27 15.25.38" style="zoom:50%;" data-proofer-ignore> <em>目标函数</em></p><p>当每一个数据都有相同的权重时，<em>quantile sketch</em> 可以解决这问题。然而目前没有算法解决<strong>加权数据</strong>，因此提出了有理论支撑的、可以解决加权数据的 <em>a novel distributed eighted quantile sketch algorithm</em> ，大概思路就是一个数据结构支持 <em>merge</em> 和 <em>prune</em> 操作，每个操作都保持一定的准确率，具体的描述和细节在论文附录中。</p><h3 id="sparsity-aware-split-finding"><span class="mr-2">Sparsity-aware Split Finding</span><a href="#sparsity-aware-split-finding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>在实际问题中，很多输入是 <strong>sparse</strong> 的，有以下几个原因：数据缺失值、很多零值、特征工程的特性（onehot）。因此提出了为每个树节点增加默认的方向 <strong>default direction</strong>，如下图。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 15.41.21.png" alt="截屏2022-03-27 15.41.21" style="zoom:50%;" data-proofer-ignore> <em>default direction</em></p><p>当一个值缺失时，会划分到默认的方向。最优的默认方向是从数据中学习而来，如下图 Algorithm 3，$x_{ik}$ 是 no-missing 的数据，只对这些不缺失的数据进行 accumulate，根据这些数据进行划分，不管 missing 数据。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 15.44.50.png" alt="截屏2022-03-27 15.44.50" style="zoom:50%;" data-proofer-ignore> <em>sparsity-aware split finding</em></p><p>实验证明，处理了这些 sparse 数据之后，快了 50 倍，证明了这个处理是有必要的。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 15.53.09.png" alt="截屏2022-03-27 15.53.09" style="zoom:50%;" data-proofer-ignore> <em>sparse实验</em></p><hr /><h2 id="system-design"><span class="mr-2">System Design</span><a href="#system-design" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="column-block-for-parallel-learning"><span class="mr-2">Column Block for Parallel Learning</span><a href="#column-block-for-parallel-learning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>树结构优化最耗时的部分就是<strong>数据排序</strong>，提出了将数据存在一种 in-memory units, <em>block</em>，数据在每一个 <em>block</em> 中是按照 <em>compressed column (CSC)</em> 格式存储的，每一个 column 按照对应的特征值进行排序，该排序能复用。</p><p>在 exact greedy algorithm 中，把整个数据集存到一个 blcok 中，然后二分搜索已经提前完成排序的 entries，一次遍历就可以收集到所有叶分支的 split candidates 的 gradient 信息。下图展示了如何将数据集传到 CSC 格式，以及利用 block 结构找到最优分解:</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 16.02.57.png" alt="截屏2022-03-27 16.02.57" style="zoom:50%;" data-proofer-ignore> <em>block structure for parallel learning</em></p><p>这个 block 结构也帮助 approximate algorithms。使用多个 blocks，每个 block 对应着数据子集，不同 block 可以分布在不同的机器上。使用这种已经完成排序的结构，<em>quantile finding step</em> 线性的遍历排好序的 columns。这个对 local proposal algorithms 很有帮助，因为 candidates 很频繁的生成。</p><p><strong>时间复杂度分析</strong>：$d$ 为树的最大深度，$K$ 为树的个数。</p><ul><li>对于 <strong>exact greedy algorithm</strong>，原始的 sparse aware algorithm 的时间复杂度是 \(O\left(K d\|\mathbf{x}\|_{0} \log n\right)\) ，这里用 \(\|\mathbf{x}\|_{0}\) 表示非缺失值的个数。在 block structure 上 tree boosting 时间复杂度为 \(O\left(K d\|\mathbf{x}\|_{0}+\|\mathbf{x}\|_{0} \log n\right)\) 这里的 \(\|\mathbf{x}\|_{0} \log n\) 就是 block structure 只计算一遍排序的时间。当需要多次排序时 block strcuture 可以节省很多计算。<li>对于 <strong>approximate algorithm</strong> 来说，用二分搜索原始的时间复杂度是 \(O\left(K d\|\mathbf{x}\|_{0} \log q\right)\) ，这里的 log 参数 $q$ 通常是在32到100之间；使用 block structure 之后，可以把时间降到 \(O\left(K d\|\mathbf{x}\|_{0}+\|\mathbf{x}\|_{0} \log B\right)\) ，这里也同样的是把只排序一遍。</ul><h3 id="cache-aware-access"><span class="mr-2">Cache-aware Access</span><a href="#cache-aware-access" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>block structure 对搜索有帮助，但对于 gradient statistics 来说还是需要按行来读取。非连续内存的读写会拖慢 split finding 的速度。对于 exact greedy algorithm 来说，可以通过 cache-aware prefetching algorithm 缓解这个问题；具体而言，给每个线程分配了连续的 buffer 用于存储 gradient statistics，使其可以在连续内存中读取信息，然后用 mini-batch 方法进行 accumulation，在数据较多的情况下减少运行时间。如图 7 显示，大数据集情况下运行时间快了两倍。</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 16.46.34.png" alt="截屏2022-03-27 16.46.34" style="zoom:50%;" data-proofer-ignore> <em>Cache-aware Access</em></p><p>对于 approximate algorithms，通过选择正确的 block size 去解决，这里的 block size 是指每个 block 最多可以存储数据的个数。选择过小的 block size 会减小每个线程的负担导致不能高效的并行运算。过大的 block size 会导致缓存不够，命中率低。经过对比几次实验后，发现每个 block 存储 $2^{16}$ 个 examples 可以很好的平衡 cache property 和 parallelizattion。</p><h3 id="blocks-for-out-of-core-computation"><span class="mr-2">Blocks for Out-of-core Computation</span><a href="#blocks-for-out-of-core-computation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>如何将一个机器的所有资源都充分利用呢？除了处理器和内存外，还可以使用<strong>磁盘空间</strong>去处理不在主内存中的数据。为了实现 <strong>out-of-core computation</strong> ，把数据分为多个 blocks 存到磁盘中，在计算的同时，使用单独的线程去 pre-fetch 这些 block 到内存中，因此 computation 可以在并行读取 disk 中发生。然而，这也不能完全解决问题，disk 的读取会耗费很长的 computation time，要想办法减小开销和增加 disk IO 的吞吐量，主要用了两个技术提升 out-of-core-computation。</p><ul><li><strong>Block Compression</strong>：block 按列压缩，并由独立的线程在加载到主内存时解压缩。对于 row index，只记录对于 beginning index 的偏移量，用 16 bit integer 存储。这里每一个 block 可以存储 $2^{16}$ 个 examples，压缩率达到 26% 到 29%。<li><strong>Block Sharding</strong>：将数据拆分到多个磁盘中，为每个磁盘分配一个 prefetch 线程，将数据取到内存缓冲区中。然后，线程交替地从每个缓冲区读取数据。当有多个磁盘可用时，这有助于提高磁盘读取的吞吐量。</ul><hr /><h2 id="related-works"><span class="mr-2">Related Works</span><a href="#related-works" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>论文中按照各个技术列了一下对应的相关工作…</p><ul><li><p><strong>Gradient boosting</strong>: additive optimization in functional space</p><li><p><strong>Regularized model</strong>: prevent overfitting</p><li><p><strong>Column sampling</strong>: prevent overfitting</p><li><p><strong>Sparsity-aware learning</strong>: handle all kinds of sparsity patterns</p><li><p><strong>Parallel tree learning</strong></p><li><p><strong>Cache-aware learning</strong></p><li><p><strong>Out-of-core computation</strong></p><li><p><strong>Weighted quantile sketch</strong>: finding quantiles on weighted data</p><p><img data-src="https://github.com/JuhaoLiang1997/JuhaoLiang1997.github.io/raw/main/_images//2022-03-27-xgboost-a-scalable-tree-boosting-system/截屏2022-03-27 17.23.39.png" alt="截屏2022-03-27 17.23.39" style="zoom:50%;" data-proofer-ignore> <em>related works</em></p></ul><hr /><h2 id="end-to-end-evaluation"><span class="mr-2">End to End Evaluation</span><a href="#end-to-end-evaluation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>这一部分就是用 XGBoost 做实验了</p><hr /><h2 id="conclusion"><span class="mr-2">Conclusion</span><a href="#conclusion" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><p>In this paper, we described the lessons we learnt when building XGBoost, a <strong>scalable tree boosting system</strong> that is widely used by data scientists and provides state-of-the-art results on many problems. We proposed a novel <strong>sparsity aware algorithm</strong> for handling sparse data and a theoretically justified <strong>weighted quantile sketch</strong> for approximate learning. Our experience shows that <strong>cache access patterns</strong>, <strong>data compression</strong> and <strong>sharding</strong> are essential elements for building a scalable end-to-end system for tree boosting. These lessons can be applied to other machine learning systems as well. By combining these insights, XGBoost is able to solve real world scale problems using a minimal amount of resources.</p></blockquote><hr /><h2 id="读后笔记"><span class="mr-2">读后笔记</span><a href="#读后笔记" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>XGBoost 支持了多类型分类器，传统的GBDT采用CART作为基学习器<li>XGBoost 支持自定目标函数，只要一阶二阶可导<li>XGBoost 增加了正则项：叶子结点数量、叶子结点值的和。<li>shrinkage 降低每棵树的提升效果，相当于学习率<li>column subsampling 列抽样，防止过拟合，加快运算<li>对所有 sparsity patterns 可以计算 default direction<li>基于特征的并行运算，在树进行分裂时，并行计算各个特征的增益<li>近似分裂算法：在树进行分裂时，可以不去遍历所有情况，而是采用近似划分的方法计算<li>数据排序实现计算并存储于 block 中，加快效率，方便并行运算<li>cache-aware 和 Out-of-core computation，利用 cache 和 disk 进行加速<li>做了很多工程上的优化</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/papers/'>papers</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/xgboost/" class="post-tag no-text-decoration" >XGBoost</a> <a href="/tags/model-ensemble/" class="post-tag no-text-decoration" >Model Ensemble</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=XGBoost: A Scalable Tree Boosting System - JuhaoLiang&amp;url=/posts/xgboost-a-scalable-tree-boosting-system/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=XGBoost: A Scalable Tree Boosting System - JuhaoLiang&amp;u=/posts/xgboost-a-scalable-tree-boosting-system/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=/posts/xgboost-a-scalable-tree-boosting-system/&amp;text=XGBoost: A Scalable Tree Boosting System - JuhaoLiang" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tools/">tools</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/model-ensemble/">Model Ensemble</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/transfer-learning/">Transfer Learning</a> <a class="post-tag" href="/tags/xgboost/">XGBoost</a> <a class="post-tag" href="/tags/gcn/">GCN</a> <a class="post-tag" href="/tags/gradient-boost/">Gradient Boost</a> <a class="post-tag" href="/tags/script/">script</a> <a class="post-tag" href="/tags/semi-supervised-learning/">Semi-supervised Learning</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/ensemble/"><div class="card-body"> <em class="timeago small" data-ts="1648307700" > 2022-03-26 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Ensemble</h3><div class="text-muted small"><p> 准备开始 Kaggle，因此学习梳理一下 ensemble 的方法。 Keywords: Model ensemble, Adaboost, Gradient Boost, Random Forest 参考资料： ML Lecture 22: Ensemble，集成学习算法总结—-Boosting和Bagging, XGBoost的技术剖析, 从决策树到XGBoost Ensembl...</p></div></div></a></div><div class="card"> <a href="/posts/graph-convolutional-networks-for-text-classification/"><div class="card-body"> <em class="timeago small" data-ts="1647785700" > 2022-03-20 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Graph Convolutional Networks for Text Classification</h3><div class="text-muted small"><p> Paper 阅读笔记 Graph Convolutional Netowrks for Text Classification 构建 双层 GCN 异构图，words 和 documents 做节点，words 之间的 edge 用词共现信息表示，word 和 documents 之间的 edge 用 tf-idf 表示。然后做把文本分类转换为点分类，该方法可以在小比例的标记文档中实现强...</p></div></div></a></div><div class="card"> <a href="/posts/transformer-attention-is-all-you-need/"><div class="card-body"> <em class="timeago small" data-ts="1648734120" > 2022-03-31 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Transformer: Attention Is All You Need</h3><div class="text-muted small"><p> Paper 阅读笔记 Attention Is All You Need, 【機器學習2021】Transformer, transformer implementation, 代码笔记, Transformer 面经总结 transformer 主要还是利用了 self-attention 的机制，打破了传统 rnn 基于序列的线性训练方法，增强其并行运算能力，克服长距离依赖问题；但...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/ensemble/" class="btn btn-outline-primary" prompt="Older"><p>Ensemble</p></a> <a href="/posts/transformer-attention-is-all-you-need/" class="btn btn-outline-primary" prompt="Newer"><p>Transformer: Attention Is All You Need</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/JuhaoLiang1997">JuhaoLiang</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tools/">tools</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/model-ensemble/">Model Ensemble</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/transfer-learning/">Transfer Learning</a> <a class="post-tag" href="/tags/xgboost/">XGBoost</a> <a class="post-tag" href="/tags/gcn/">GCN</a> <a class="post-tag" href="/tags/gradient-boost/">Gradient Boost</a> <a class="post-tag" href="/tags/script/">script</a> <a class="post-tag" href="/tags/semi-supervised-learning/">Semi-supervised Learning</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-77HPQKSLY9"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-77HPQKSLY9'); }); </script>
