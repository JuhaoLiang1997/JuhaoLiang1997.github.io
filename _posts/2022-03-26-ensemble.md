---
layout: post
title: Ensemble
date: 2022-03-26 23:15 +0800
category: [Technical]
tags: [Model Ensemble, Gradient Boost, XGBoost]
typora-root-url: "../_images"
math: true
comments: false
toc: true
pin: false
---

准备开始 Kaggle，因此学习梳理一下 ensemble 的方法。

**Keywords**: Model ensemble, Adaboost, Gradient Boost, Random Forest

**参考资料**： [ML Lecture 22: Ensemble](https://www.youtube.com/watch?v=tH9FH1DH5n0)，[集成学习算法总结----Boosting和Bagging](https://blog.csdn.net/a1b2c3d4123456/article/details/51834272), [XGBoost的技术剖析](https://www.bilibili.com/video/BV1si4y1G7Jb?share_source=copy_web),  [从决策树到XGBoost](https://zhuanlan.zhihu.com/p/58269560)

## Ensemble 方法梳理

### 概述

- Ensemble 就是针对一个任务，训练多个模型，最后把各个学习结果加权输出。
- 而实际上ensemble的主要技巧就是在保持模型相对精度的情况下，如何增加个体之间的差异度。
- 多个弱分类器的组合，一般有简单多数投票、权重投票，贝叶斯投票，基于D-S证据理论的整合，基于不同的特征子集的整合。
- 一般采用弱分类器的原因在于将误差进行均衡，因为一旦某个分类器太强了就会造成后面的结果受其影响太大，严重的会导致后面的分类器无法进行分类。常用的弱分类器可以采用误差率小于0.5的，比如说逻辑回归、SVM、神经网络。
- **sklearn**: [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), [AdaBoostRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html), [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) 

### 两个类别

1. Bagging (Bootstrap AGgregation)

   - 例如：随机森林
   - 采用的是随机有放回的选择训练数据（bootstrap sampling）然后构造分类器，最后组合；不容易 overfitting。
   - 子模型是 overfitting 的，训练 models 是无序的（并行）。

2. Boosting

   - 例如：Adaboost, Gradient Boost Decision Trees (GBDT), XGBoost
   - Boosting 是一种迭代算法，根据上一次迭代的预测结果进行优化（Adaboost 是调整样本权重，XGBoost是补充残差），从而减小误差，降低模型的bias。是基于残差的训练。
   - 子模型是 underfitting 的，训练 models 是有顺序的。
   
   > Guarantee: if your ML algorithm can produce classifier with error rate smaller than 50% on training data, you can obtain 0% error rate classifier after boosting.

---

### 获取不同的数据集

- Re-sampling 用不同的数据子集
- Re-weighting 修改数据的权重，改 loss function 即可

<img src="/2022-03-26-ensemble/截屏2022-03-26 15.43.08.png" alt="截屏2022-03-26 15.43.08" style="zoom:50%;" />

---

## Models

### Adaboost

- Idea: training $f_{2}(x)$ on the new training set (re-weighted set) that fails $f_{1}(x)$ 

- 先将 $f_{1}$ 训练一遍，得到 error rate，再根据他的 prediciton 进行数据 weighting，使其 error rate 等于 0.5，再用 $f_{2}$ 在 weighted training set 上训练，这时 $f_{2}$ 就相当于补了 $f_{1}$ 的缺点。

- Re-weighting 方法，分类错的数据 weight 增加，分类对的数据 weight 减小
	
	<img src="/2022-03-26-ensemble/截屏2022-03-26 15.53.06.png" alt="截屏2022-03-26 15.53.06" style="zoom:50%;" />
	
- 上图 d1 通过计算得到值，使其 reweight 之后 error rate 恰好等于 0.5：分类正确的权重和**等于**分类错误的权重和，下图 $\varepsilon_{1} $ 是 $f_{1}$ 分类错误的数据权重**除以**总权重。

  <img src="/2022-03-26-ensemble/截屏2022-03-26 15.58.41.png" alt="截屏2022-03-26 15.58.41" style="zoom:50%;" />

  <img src="/2022-03-26-ensemble/截屏2022-03-26 16.12.44.png" alt="截屏2022-03-26 16.12.44" style="zoom:50%;" />

  <img src="/2022-03-26-ensemble/截屏2022-03-26 16.12.30.png" alt="截屏2022-03-26 16.12.30" style="zoom:50%;" />
  
- How to aggregate all classifier? 错误率低的 classifier 有较大的 $\alpha$ ，错误率低的 classifier 在 aggregate 中有较高权重。

  <img src="/2022-03-26-ensemble/截屏2022-03-26 16.15.16.png" alt="截屏2022-03-26 16.15.16" style="zoom:50%;" />

- 即使 training data 已经 fit 了，不断增加 classifier 会继续增加 margin，使 model 更加 robust

---

### Gradient Boosting

- Gradient Boosting 的主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向，使得新的 Loss 最小。

- 可以定任意的 objective function，如下图的 $l\left(\hat{y}_{0}^{n}, g\left(x^{n}\right)\right)$，就是 general 的 adaboost。

- 新的模型 $f_{t}(x)$ 是和之前所有模型的集成模型 $g_{t-1}(x)$ 是互补的，从而得到新的更好的集成模型 $g_{t}(x)$ 。怎么判断 $g_{t}(x)$ 是不是优秀呢，需要定义一个 objective function。

  <img src="/2022-03-26-ensemble/截屏2022-03-26 18.24.14.png" alt="截屏2022-03-26 18.24.14" style="zoom:50%;" />

  <img src="/2022-03-26-ensemble/截屏2022-03-26 18.19.49.png" alt="截屏2022-03-26 18.19.49" style="zoom:50%;" />

  <img src="/2022-03-26-ensemble/截屏2022-03-26 18.29.18.png" alt="截屏2022-03-26 18.29.18" style="zoom:50%;" />
  
- 这里推导可以参考 [ML Lecture 22: Ensemble](https://www.youtube.com/watch?v=tH9FH1DH5n0) 1:23:00 左右

---

### XGBoost (eXtremeGradient Boosting)

- CART 分类回归树（classification and regression tree）是 xgboost 的基础模型，使用基尼系数计算增益进行特征选取，基尼系数代表着模型的不纯度，**基尼系数越小，不纯度越低，特征越好**，和信息增益率相反。

  <img src="/2022-03-26-ensemble/截屏2022-03-26 20.51.25.png" alt="截屏2022-03-26 20.51.25" style="zoom:40%;" />

  其中， $p(k)$ 是分类 $k$ 出现的概率，K 是分类的数目。Gini(D) 反映了从数据集 D 中随机抽取两个样本，其类别标记不一致的概率。因此，**Gini(D)越小，则数据集D的纯度越高**。

  <img src="/2022-03-26-ensemble/截屏2022-03-26 20.53.26.png" alt="截屏2022-03-26 20.53.26" style="zoom:40%;" />

- additive training **叠加式训练**，预测结果由 K 棵树结果相加，即第 $k$ 棵树的结果应该是第 $k-1$ 棵树的结果与真实值的差。

  <img src="/2022-03-26-ensemble/截屏2022-03-26 21.42.43.png" alt="截屏2022-03-26 21.42.43" style="zoom:50%;" />

- 目标函数 **Objective function = loss + 惩罚项 penalty/regularization**，惩罚项是为了降低模型复杂度，防止过拟合，这里就是控制每棵树的复杂度（叶节点个数/树的深度/叶节点值）

  <img src="/2022-03-26-ensemble/截屏2022-03-26 21.44.44.png" alt="截屏2022-03-26 21.44.44" style="zoom:50%;" />

- 用 [泰勒级数](https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0) 近似目标函数，从而简化目标函数

  <img src="/2022-03-26-ensemble/截屏2022-03-26 22.14.10.png" alt="截屏2022-03-26 22.14.10" style="zoom:50%;" />

- 树的复杂度，第一项是叶的节点数，$\gamma$ 控制权重，第二项是叶的值，$\lambda$ 控制权重。在选择树的形状时，通过树的复杂度的计算来选择特征分裂，从而获得复杂度最低的树。

  <img src="/2022-03-26-ensemble/截屏2022-03-26 22.40.18.png" alt="截屏2022-03-26 22.40.18" style="zoom:50%;" />

---

### Random Forest (随机森林)

Random Forest: Bagging of decision tree, 

- 利用 **bootstrap sampling** 取样本训练多个决策树，最终结果由所有决策树投票得出

- 优点：可以处理高维数据、可以判断特征重要性以及特征之间的联系、不容易过拟合、可以处理不平衡数据集

- 缺点：在噪音较大的分类或回归问题上会过拟合、比决策树计算成本更高

- 随机森林有效缓解 overfitting 主要依靠了其中三个**随机过程**，即产生决策树的样本是随机生成，构建决策树的特征值是随机选取，树产生过程中裂变的时候是选择N个最佳方向中的随机一个裂变的。当随机森林产生的树的数目趋近无穷的时候，理论上根据大数定理可以证明训练误差与测试误差是收敛到一起的。[link](https://www.zhihu.com/question/30295075/answer/139494831)

  > 对于单个决策树模型，每次分裂时根据信息增益/信息增益比/基尼指数选择最好的特征进行分裂

- **bootstrap sampling**：有放回的采样，会导致约有36%的样本永远不会被采样到。

  > 假设有m个样本，有放回的采样，每次被采样到的概率是(1/m),每次不被采样到的概率是(1-1/m)；则(1-1/m)的n次方，当n足够大是，极限为1/e（约等于36%）。

- **out-of-bag (oob)**：根据 bootstrap sampling，使用没被训练过的 36%的样本作为验证集，求得的误差就是 out-of-bag error

```python
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(random_state=0, oob_score=True)
clf.fit(x_data, y_data)
print(clf.oob_score_)
```

