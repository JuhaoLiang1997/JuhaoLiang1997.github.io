---
layout: post
title: 关于自监督学习模型的随笔
date: 2022-05-16 20:38 +0800
category: [Technical]
tags: [BERT, NLP, Transfer Learning]
typora-root-url: "../_images"
math: true
comments: false
toc: true
pin: false
---

# 关于自监督学习模型的随笔

吃完饭休息一会，看李宏毅老师视频做的随笔

**参考资料**：[惡搞自督導式學習模型 BERT 的三個故事](https://youtu.be/Pal2DbmiYpk), [Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model](https://arxiv.org/abs/1909.09587)

---

## Overview

视频讲述了 BERT 三种有意思的研究：

- Cross-lingual
- Cross-discipline
- Pre-training with artificial data

第一点是关于多语言 BERT 的跨语言能力，例如不同语言通过向量转换进行的字符级翻译，这个是符合直觉的方法，而且是很有意思的研究。

第二点是关于 BERT 跨学科能力，感觉是从 cross-lingual 进阶而来，大概思路就是将其他任务输出为伪 "token" 形式，与已有的语言 token 做一个映射，从而利用了模型中语言预训练的信息。

第三点是尝试使用不同规则下人为制造的信息进行预训练，查看对不同任务的提升效果。

总的来说，这三点是层层递进的，主要做的也都是做迁移学习 transfer learning，利用不同领域的数据进行互相协助。Transfer Learning + Multimodal machine learning is promising.

---

## Cross-lingual Capability of BERT

多语言 BERT 的 跨语言能力，只用英文数据集 SQuAD fine tune 的模型在中文问答数据集 DRCD 有不俗的效果。

> XTREME: Cross-lingual TRansfer Evaluation of Multilingual Encoders benchmark 用来测试跨语言表达能力的数据集

<img src="/2022-05-16-关于自监督学习模型的随笔/Screenshot 2022-05-16 at 13.44.00.png" alt="Screenshot 2022-05-16 at 13.44.00" style="zoom:50%;" />
_Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model_

通过一个 bi-lingual dictionary 得到中英字符对应关系，在 BERT 中计算中英字符各自的 embedding（corpus平均），再计算中英对应关系字符的 embedding similarity，做 ranking，得到 Mean Reciprocal Rank (MRR)，越高 MRR 表示越好的跨语言表示能力。

<img src="/2022-05-16-关于自监督学习模型的随笔/Screenshot 2022-05-16 at 13.55.51.png" alt="Screenshot 2022-05-16 at 13.55.51" style="zoom:40%;" />
_Mean Reciprocal Rank_

将 Multi-lingual BERT 所有中文符号 embedding 和英文符号 embedding 分别做平均，得到两个语言的 average embedding，两个语言的平均向量的差可以近似的理解为两个语言的转换方式，例如 embedding(fish) + embedding(中文) - embedding(English) = embedding(鱼)。

---

## Cross-discipline Capability of BERT

BERT 的跨学科能力，在人类语言学习的知识对其他学科有一定的帮助，例如将 DNA 序列元素一一映射到语言元素之后，使用语言数据集预训练的 BERT 对 DNA 分类有一定的帮助。通过语言数据的预训练，对其他学科的应用是有帮助的 both Optimization and Generalization. 

BERT 的划学科能力对 Speech Question Answering 有帮助。Speech Question Answering 任务就是输入一段语音，输出答案所在位置。同样是将语音的 token 和 语言的 token 一一对应，再接上语言数据集预训练的模型，可以有效提高性能。

<img src="/2022-05-16-关于自监督学习模型的随笔/Screenshot 2022-05-16 at 14.27.35.png" alt="Screenshot 2022-05-16 at 14.27.35" style="zoom:40%;" />
_pretrained speech model + pretrained text BERT_

---

## Pre-training with Artificial Data of BERT

除了语言数据，在基于规则的人造数据上预训练 BERT。如果是随机生成的人造数据，则对实际任务中并无作用；而如果有一定规律的（成对出现），则会有一定的帮助；shuffle 打乱连续编号序列，也会对结果有帮助。
